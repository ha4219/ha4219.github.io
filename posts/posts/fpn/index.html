<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="generator" content="Gatsby 5.3.3"/><style data-href="/styles.07a3663f36c261d02a9a.css" data-identity="gatsby-global-css">@import url(https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.6/dist/web/static/pretendard.css);
/*
! tailwindcss v3.2.4 | MIT License | https://tailwindcss.com
*/*,:after,:before{border:0 solid #e5e7eb;box-sizing:border-box}:after,:before{--tw-content:""}html{-webkit-text-size-adjust:100%;font-feature-settings:normal;font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4}body{line-height:inherit;margin:0}hr{border-top-width:1px;color:inherit;height:0}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,pre,samp{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}table{border-collapse:collapse;border-color:inherit;text-indent:0}button,input,optgroup,select,textarea{color:inherit;font-family:inherit;font-size:100%;font-weight:inherit;line-height:inherit;margin:0;padding:0}button,select{text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button;background-color:transparent;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:baseline}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}fieldset{margin:0}fieldset,legend{padding:0}menu,ol,ul{list-style:none;margin:0;padding:0}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{color:#9ca3af;opacity:1}input::placeholder,textarea::placeholder{color:#9ca3af;opacity:1}[role=button],button{cursor:pointer}:disabled{cursor:default}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{height:auto;max-width:100%}[hidden]{display:none}html{font-family:Pretendard}*,:after,:before{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: }::backdrop{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: }.container{width:100%}@media (min-width:320px){.container{max-width:320px}}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}@media (min-width:1536px){.container{max-width:1536px}}.sr-only{clip:rect(0,0,0,0);border-width:0;height:1px;margin:-1px;overflow:hidden;padding:0;white-space:nowrap;width:1px}.absolute,.sr-only{position:absolute}.relative{position:relative}.bottom-0{bottom:0}.float-right{float:right}.float-left{float:left}.mx-auto{margin-left:auto;margin-right:auto}.my-1{margin-bottom:.25rem;margin-top:.25rem}.mt-4{margin-top:1rem}.mr-3{margin-right:.75rem}.ml-3{margin-left:.75rem}.mt-1{margin-top:.25rem}.mr-1{margin-right:.25rem}.ml-1{margin-left:.25rem}.block{display:block}.flex{display:flex}.inline-flex{display:inline-flex}.hidden{display:none}.h-5{height:1.25rem}.h-10{height:2.5rem}.h-6{height:1.5rem}.h-24{height:6rem}.h-20{height:5rem}.w-full{width:100%}.w-5{width:1.25rem}.w-6{width:1.5rem}.w-20{width:5rem}.flex-col{flex-direction:column}.flex-wrap{flex-wrap:wrap}.items-center{align-items:center}.justify-center{justify-content:center}.justify-between{justify-content:space-between}.space-x-6>:not([hidden])~:not([hidden]){--tw-space-x-reverse:0;margin-left:calc(1.5rem*(1 - var(--tw-space-x-reverse)));margin-right:calc(1.5rem*var(--tw-space-x-reverse))}.self-center{align-self:center}.truncate{overflow:hidden;text-overflow:ellipsis}.truncate,.whitespace-nowrap{white-space:nowrap}.rounded-lg{border-radius:.5rem}.rounded{border-radius:.25rem}.border{border-width:1px}.border-gray-200{--tw-border-opacity:1;border-color:rgb(229 231 235/var(--tw-border-opacity))}.border-gray-100{--tw-border-opacity:1;border-color:rgb(243 244 246/var(--tw-border-opacity))}.bg-white{--tw-bg-opacity:1;background-color:rgb(255 255 255/var(--tw-bg-opacity))}.bg-gray-50{--tw-bg-opacity:1;background-color:rgb(249 250 251/var(--tw-bg-opacity))}.bg-gray-100{--tw-bg-opacity:1;background-color:rgb(243 244 246/var(--tw-bg-opacity))}.object-cover{-o-object-fit:cover;object-fit:cover}.p-4{padding:1rem}.p-2{padding:.5rem}.px-2{padding-left:.5rem;padding-right:.5rem}.py-2{padding-bottom:.5rem;padding-top:.5rem}.px-10{padding-left:2.5rem;padding-right:2.5rem}.pl-3{padding-left:.75rem}.pr-4{padding-right:1rem}.pt-10{padding-top:2.5rem}.text-3xl{font-size:1.875rem;line-height:2.25rem}.text-sm{font-size:.875rem;line-height:1.25rem}.text-xs{font-size:.75rem;line-height:1rem}.font-bold{font-weight:700}.font-semibold{font-weight:600}.text-gray-500{--tw-text-opacity:1;color:rgb(107 114 128/var(--tw-text-opacity))}.text-gray-700{--tw-text-opacity:1;color:rgb(55 65 81/var(--tw-text-opacity))}.text-gray-400{--tw-text-opacity:1;color:rgb(156 163 175/var(--tw-text-opacity))}.hover\:bg-gray-100:hover{--tw-bg-opacity:1;background-color:rgb(243 244 246/var(--tw-bg-opacity))}.hover\:text-gray-900:hover{--tw-text-opacity:1;color:rgb(17 24 39/var(--tw-text-opacity))}.hover\:underline:hover{text-decoration-line:underline}.focus\:outline-none:focus{outline:2px solid transparent;outline-offset:2px}.focus\:ring-2:focus{--tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color);--tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color);box-shadow:var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 #0000)}.dark .dark\:border-gray-700{--tw-border-opacity:1;border-color:rgb(55 65 81/var(--tw-border-opacity))}.dark .dark\:bg-gray-900{--tw-bg-opacity:1;background-color:rgb(17 24 39/var(--tw-bg-opacity))}.dark .dark\:bg-gray-800{--tw-bg-opacity:1;background-color:rgb(31 41 55/var(--tw-bg-opacity))}.dark .dark\:bg-gray-700{--tw-bg-opacity:1;background-color:rgb(55 65 81/var(--tw-bg-opacity))}.dark .dark\:text-gray-400{--tw-text-opacity:1;color:rgb(156 163 175/var(--tw-text-opacity))}.dark .dark\:text-white{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity))}.dark .dark\:hover\:bg-gray-700:hover{--tw-bg-opacity:1;background-color:rgb(55 65 81/var(--tw-bg-opacity))}.dark .dark\:hover\:text-white:hover{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity))}.dark .dark\:focus\:ring-gray-600:focus{--tw-ring-opacity:1;--tw-ring-color:rgb(75 85 99/var(--tw-ring-opacity))}@media (min-width:640px){.sm\:mt-0{margin-top:0}.sm\:flex{display:flex}.sm\:items-center{align-items:center}.sm\:justify-between{justify-content:space-between}.sm\:p-6{padding:1.5rem}.sm\:px-24{padding-left:6rem;padding-right:6rem}.sm\:text-center{text-align:center}}@media (min-width:768px){.md\:mt-0{margin-top:0}.md\:block{display:block}.md\:hidden{display:none}.md\:w-auto{width:auto}.md\:flex-row{flex-direction:row}.md\:space-x-8>:not([hidden])~:not([hidden]){--tw-space-x-reverse:0;margin-left:calc(2rem*(1 - var(--tw-space-x-reverse)));margin-right:calc(2rem*var(--tw-space-x-reverse))}.md\:border-0{border-width:0}.md\:bg-white{--tw-bg-opacity:1;background-color:rgb(255 255 255/var(--tw-bg-opacity))}.md\:bg-transparent{background-color:transparent}.md\:p-0{padding:0}.md\:text-sm{font-size:.875rem;line-height:1.25rem}.md\:font-medium{font-weight:500}.md\:text-blue-700{--tw-text-opacity:1;color:rgb(29 78 216/var(--tw-text-opacity))}.md\:hover\:bg-transparent:hover{background-color:transparent}.md\:hover\:text-blue-700:hover{--tw-text-opacity:1;color:rgb(29 78 216/var(--tw-text-opacity))}.dark .md\:dark\:bg-gray-900{--tw-bg-opacity:1;background-color:rgb(17 24 39/var(--tw-bg-opacity))}.dark .md\:dark\:bg-transparent{background-color:transparent}.dark .md\:dark\:text-white{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity))}.dark .md\:dark\:hover\:bg-transparent:hover{background-color:transparent}.dark .md\:dark\:hover\:text-white:hover{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity))}}</style><link rel="preconnect" href="https://www.google-analytics.com"/><link rel="dns-prefetch" href="https://www.google-analytics.com"/><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){const t=e.target;if(void 0===t.dataset.mainImage)return;if(void 0===t.dataset.gatsbyImageSsr)return;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><link rel="sitemap" type="application/xml" href="/sitemap-index.xml"/><link rel="icon" href="/favicon-32x32.png?v=1a681e90c6fbce95b939804447e9eddc" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=1a681e90c6fbce95b939804447e9eddc"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=1a681e90c6fbce95b939804447e9eddc"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=1a681e90c6fbce95b939804447e9eddc"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=1a681e90c6fbce95b939804447e9eddc"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=1a681e90c6fbce95b939804447e9eddc"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=1a681e90c6fbce95b939804447e9eddc"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=1a681e90c6fbce95b939804447e9eddc"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=1a681e90c6fbce95b939804447e9eddc"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div style="position:relative;min-height:100vh;padding-bottom:6rem"><header data-testid="header" class="border-gray-200 bg-white px-2 dark:border-gray-700 dark:bg-gray-900"><div class="container mx-auto flex flex-wrap items-center justify-between"><a class="flex items-center" data-testid="logo" href="/"><div data-gatsby-image-wrapper="" class="gatsby-image-wrapper gatsby-image-wrapper-constrained sm:h-15 mr-3 h-10"><div style="max-width:48px;display:block"><img alt="" role="presentation" aria-hidden="true" src="data:image/svg+xml;charset=utf-8,%3Csvg height=&#x27;48&#x27; width=&#x27;48&#x27; xmlns=&#x27;http://www.w3.org/2000/svg&#x27; version=&#x27;1.1&#x27;%3E%3C/svg%3E" style="max-width:100%;display:block;position:static"/></div><div aria-hidden="true" data-placeholder-image="" style="opacity:1;transition:opacity 500ms linear;background-color:#f8f8f8;position:absolute;top:0;left:0;bottom:0;right:0"></div><picture><source type="image/webp" data-srcset="/static/e9471ff7cc72c4b159fd9fe8f6569f14/30aa9/logoB.webp 12w,/static/e9471ff7cc72c4b159fd9fe8f6569f14/4e704/logoB.webp 24w,/static/e9471ff7cc72c4b159fd9fe8f6569f14/e78b1/logoB.webp 48w" sizes="(min-width: 48px) 48px, 100vw"/><img data-gatsby-image-ssr="" data-main-image="" style="opacity:0" sizes="(min-width: 48px) 48px, 100vw" decoding="async" loading="lazy" data-src="/static/e9471ff7cc72c4b159fd9fe8f6569f14/fcdb9/logoB.png" data-srcset="/static/e9471ff7cc72c4b159fd9fe8f6569f14/29278/logoB.png 12w,/static/e9471ff7cc72c4b159fd9fe8f6569f14/2391d/logoB.png 24w,/static/e9471ff7cc72c4b159fd9fe8f6569f14/fcdb9/logoB.png 48w" alt="logo-image"/></picture><noscript><picture><source type="image/webp" srcSet="/static/e9471ff7cc72c4b159fd9fe8f6569f14/30aa9/logoB.webp 12w,/static/e9471ff7cc72c4b159fd9fe8f6569f14/4e704/logoB.webp 24w,/static/e9471ff7cc72c4b159fd9fe8f6569f14/e78b1/logoB.webp 48w" sizes="(min-width: 48px) 48px, 100vw"/><img data-gatsby-image-ssr="" data-main-image="" style="opacity:0" sizes="(min-width: 48px) 48px, 100vw" decoding="async" loading="lazy" src="/static/e9471ff7cc72c4b159fd9fe8f6569f14/fcdb9/logoB.png" srcSet="/static/e9471ff7cc72c4b159fd9fe8f6569f14/29278/logoB.png 12w,/static/e9471ff7cc72c4b159fd9fe8f6569f14/2391d/logoB.png 24w,/static/e9471ff7cc72c4b159fd9fe8f6569f14/fcdb9/logoB.png 48w" alt="logo-image"/></picture></noscript><script type="module">const t="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;if(t){const t=document.querySelectorAll("img[data-main-image]");for(let e of t){e.dataset.src&&(e.setAttribute("src",e.dataset.src),e.removeAttribute("data-src")),e.dataset.srcset&&(e.setAttribute("srcset",e.dataset.srcset),e.removeAttribute("data-srcset"));const t=e.parentNode.querySelectorAll("source[data-srcset]");for(let e of t)e.setAttribute("srcset",e.dataset.srcset),e.removeAttribute("data-srcset");e.complete&&(e.style.opacity=1,e.parentNode.parentNode.querySelector("[data-placeholder-image]").style.opacity=0)}}</script></div><span class="text-l self-center whitespace-nowrap font-semibold dark:text-white">jeongdongha.me</span></a><button data-collapse-toggle="navbar-default" type="button" class="ml-3 inline-flex items-center rounded-lg p-2 text-sm text-gray-500 hover:bg-gray-100 focus:outline-none focus:ring-2  dark:text-gray-400 dark:hover:bg-gray-700 dark:focus:ring-gray-600 md:hidden" aria-controls="navbar-default" aria-expanded="false"><span class="sr-only">Open main menu</span><svg data-collapse-toggle="navbar-default" class="h-6 w-6" aria-hidden="true" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></button><div class="w-full md:block md:w-auto" id="navbar-default" hidden=""><ul class="mt-4 flex flex-col rounded-lg border border-gray-100 bg-gray-50 p-4 dark:border-gray-700 dark:bg-gray-800 md:mt-0 md:flex-row md:space-x-8 md:border-0 md:bg-white md:text-sm md:font-medium md:dark:bg-gray-900"><li><a class="block rounded py-2 pl-3 pr-4 text-gray-700 hover:bg-gray-100 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white md:border-0 md:p-0 md:hover:bg-transparent md:hover:text-blue-700 md:dark:hover:bg-transparent md:dark:hover:text-white" href="/about/">About</a></li><li><a class="block rounded py-2 pl-3 pr-4 text-gray-700 hover:bg-gray-100 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white md:border-0 md:p-0 md:hover:bg-transparent md:hover:text-blue-700 md:dark:hover:bg-transparent md:dark:hover:text-white" href="/projects/">Projects</a></li><li><a class="block rounded py-2 pl-3 pr-4 text-gray-700 hover:bg-gray-100 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white md:border-0 md:p-0 md:hover:bg-transparent md:hover:text-blue-700 md:dark:hover:bg-transparent md:dark:hover:text-white" href="/posts/">Posts</a></li></ul></div></div></header><div class="px-10 pt-10 sm:px-24"><h1>Feature Pyramid Networks for Object Detection</h1>
<h2>Abstract</h2>
<pre><code>Feature pyramids are a basic component in recognition
systems for detecting objects at different scales. But recent
deep learning object detectors have avoided pyramid representations, in part because they are compute and memory
intensive. In this paper, we exploit the inherent multi-scale,
pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for
building high-level semantic feature maps at all scales. This
architecture, called a Feature Pyramid Network (FPN),
shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster
R-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without
bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 6 FPS on a GPU
and thus is a practical and accurate solution to multi-scale
object detection. Code will be made publicly available.
</code></pre>
<ul>
<li>FPN은 여러가지 scale을 가진 객체탐지에 기본적인 component이다. 하지만 최근 다른 연구에서는 복잡하고 메모리 집중적이기 때문에 pyramid representations 피한다. 본 논문에서, 한계 추가 비용으로 feature pyramid를 구성하기 위해 고유의 multi-scale, 즉 pyramidal hierarchy of deep convolutional networks(convolutional network의 피라미드 계층)을 활용한다. All scale에 high-level feature maps에 대한 top-down 구조는 개발됐다.  FPN은  몇가지 application의 generic feature extractor에 중요한 성장을 보여준다. FPN을 사용한 Faster-RCNN은 without bells and whistles, COCO detection benchmark에서 최첨단? 단일 모델 결과를 달성하여 COCO 2016 challenge 우승자를 포함한 모든 기존 단일 모델 항목을 능가한다. 추가로 우리 모델은 GPU 환경에서 6 FPS를 갖는다. 따라서 multi-scale 객체탐지에 실용적이고 정확한 해결책이다. code는 곧 공식적으로 이용 가능하다.</li>
</ul>
<h2>1. Introduction</h2>
<pre><code> Recognizing objects at vastly different scales is a fundamental challenge in computer vision. Feature pyramids built upon image pyramids (for short we call these featurized image pyramids) form the basis of a standard solution [1] (Fig. 1(a)). These pyramids are scale-invariant in the sense that an object’s scale change is offset by shifting its
level in the pyramid. Intuitively, this property enables a model to detect objects across a large range of scales by scanning the model over both positions and pyramid levels.
 Featurized image pyramids were heavily used in the era of hand-engineered features [5, 25]. They were so critical that object detectors like DPM [7] required dense scale sampling to achieve good results (e.g., 10 scales per octave). For recognition tasks, engineered features have largely been replaced with features computed by deep convolutional networks (ConvNets) [19, 20]. Aside from being capable of representing higher-level semantics, ConvNets are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)). But even with this robustness, pyramids are still needed to get the most accurate results. All recent top entries in the ImageNet [33] and COCO [21] detection challenges use multi-scale testing on featurized image pyramids (e.g., [16, 35]). The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.
 Nevertheless, featurizing each level of an image pyramid has obvious limitations. Inference time increases considerably (e.g., by four times [11]), making this approach
impractical for real applications. Moreover, training deep 
</code></pre>
<ul>
<li>
<p>매우 다른 sacles에 객체를 인식하는 것은 CV에 기초적인 과제이다. image pyramids에서 형성된 Feature pyramids는 기본적인 해결책의 근간을 형성한다.  pyramids는 물체의 스케일 변화가 다음 값을 이동함으로써 상쇄된다는 점에서 scale-불변이다. 직관적으로, this property를 사용하면 모델이 위치 및 피라미드 레벨 모두에서 모델을 스캔하여 광범위한 축척에 걸쳐 객체를 탐지할 수 있습니다.</p>
</li>
<li>
<p>Featurized image pyramids는 hand-engineered features에서 많이 사용된다. DPM과 같은 object detectors가 좋은 결과를 내기 위해 dense scale sampling이 필요한 것은 치명적이다. 인식 과제에서, engineered된 feature들은 deep ConvNets으로 계산된 features으로 바뀌었다. 더 높은 semantics를 나타낼 뿐만 아니라, ConvNets scale 변화에 강력하다. 따라서 single input scale에서 계산된 features로 인식을 용이하게 한다. 강건하지만 pyramids는 여전히 가장 좋은 정확도 결과를 필요로 한다. 최근 유명 대회에 참가하는 모델들은 featurized image pyramids에 multi-scale testing을 사용한다. 각 level에 image pyramid를 featurizing의  주요 이점은 고해상도를 포함한 모든 level에서 semantically strong한 multi-scale feature 를 제공한다.</p>
<img src="C:%5CUsers%5Cha421%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220113095136558.png" alt="image-20220113095136558"/>
</li>
<li>
<p>그럼에도 불구하고 명백한 한계가 존재한다. 시간이 상당히 증가하고 실제 app에서 비실용적일 것이라고 예측된다. 게다가 end-to-end networks에 올리는 것은 메모리적으로 실현 불가능하고 image pyramid는 오직 test time에 사용되기에 train과 test-time 추론 사이에 모순이 발생한다. 이러한 이유로 Fast, Faster R-CNN은 featurized image pyramids를 기본 세팅에 사용하지 않기로 선택했다.</p>
</li>
<li>
<p>하지만 image pyrmaid는 오직 multi-scale feature representation을 계산하는 것만 있는게 아니다. deep ConvNet은 각 layer마다 feature hierarchy를 계산하고 feature 계층을 subsampling은 고유한 multi-scale과 pyramidal shape를 갖고 있다. 이러한 feature hierarchy는 각기 다른 해상도의 feature map을 제공하지만 깊이에 따라 야기되는 큰 semantic gaps을 보여준다. 높은 해상도 maps은 인식 능력에 안좋은 낮은 feature를 갖고있다.</p>
</li>
<li>
<p>SSD는 ConvNet의 마치 featurized image pyramid처럼 pyramidal feature hierarchy를 처음 사용한 시도 중 하나이다. 이상적으로 SSD-style pyramid는 다른 여러 이전에 통과된 layer로 부터 multi-scale feature maps을 재사용해서 cost가 적다. 그러나 low-level feature을 사용하지 않으려면 SSD는 이미 계산된 hierarchy를 다시 사용하고 대신 새로운 layer를 추가하면서 network의 상위 계층으로부터 pyramid를 구축한다. 따라서 고해상도 계층맵을 사용할 기회를 잃는다. 고해상도 맵은 작은 객체를 탐지할 때 중요하다.</p>
</li>
<li>
<p>이 논문의 목표는 모든 scale에 강력한 ConvNet&#x27;s feature hierarchy를 만들어서 pyramid shape에 영향을 주는 것이다. 목표를 달성하려면 낮은 해상도와 높은 해상도의 semantically 한 강한 features 그리고 top-down pathway와 lateral connections을 통과하고 semantically하고 약한 features들 구조에 달려있다.</p>
</li>
</ul>
<h2><a href="https://youtu.be/r3U9MJslg5g">Ref</a></h2>
<p>정말 많이 보고 배워서 참고합니다. 또한 영상 내용을 시간을 바탕으로 제가 새로 재구성했습니다.</p>
<ul>
<li>
<p><strong>background</strong></p>
<ul>
<li>
<p><strong>Resolution &amp;&amp; level feature</strong></p>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932669533649535026/unknown.png" alt="img"/>
<p>초반 layer에서는 높은 해상도 를 갖고 있지만 각 pixel하나는 큰 의미가 없을 수 있다. (매우 작은 object면 있을 수도 있음. 하지만 대부분 object들은 거의 pixel 하나에만 들어있지 않고 across하기 때문에 그럴리는 적다.) FC를 제외하고 뒤로가면 갈수록 해상도는 낮아지지만 depth가 깊어지고 각 pixel에 담는 정보는 매우 의미있음을 알 수 있다.</p>
<p>여기서 pixel은 x.shape = (512,  w, h)로 봤을때 x[:, p_x, p_y] 값을 의미한다. 또한 FC를 제외하고 생각하자.</p>
</li>
<li>
<p><strong>Scale</strong></p>
<ul>
<li>
<p>Small Scale =&gt; 나무</p>
<img src="C:%5CUsers%5Cha421%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220118004214806.png" alt="image-20220118004214806"/>
</li>
<li>
<p>Large Scale =&gt; 숲</p>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932661190805561415/unknown.png" alt="img"/>
</li>
<li>
<p>비교</p>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932662734804357160/unknown.png" alt="img"/>
<p>그러면 객체를 인식하기 위해서는 Large Scale을 만드는 것이 좋을 것이다. 그러기 위한 방법으로는 2가지가 있다.</p>
<ol>
<li>Window Size를 늘린다. (맨 오른쪽 그림)</li>
<li>Image Size를 줄인다. (3번째 그림)</li>
</ol>
<p>그림에서는 window 크기를 늘린 것과 image 크기를 줄이는게 차이가 있어보이지만 사실은 차이가 없다. 의미만 부여하기 위해 만들었다.</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Paper list</strong></p>
<img src="https://github.com/hoya012/deep_learning_object_detection/raw/master/assets/deep_learning_object_detection_history.PNG" alt="img"/>
</li>
<li>
<p><strong>Object Detection Milestones</strong></p>
<img src="https://user-images.githubusercontent.com/31475037/75324222-d2612f80-58b9-11ea-8ae5-e1ad0e1ccfd5.png" alt="img"/>
</li>
<li>
<p>이전에 object detection에는 one-stage detector와 two-stage dectector가 있다고 소개했습니다.</p>
<ul>
<li><strong>1-Stage Detector</strong>는 Region Proposal -&gt; Classification이 순서대로 실행</li>
<li><strong>2-Stage Detector</strong>는 Region Proposal, Classification이 동시에 실행</li>
</ul>
</li>
</ul>
<h2>FPN</h2>
<img src="https://production-media.paperswithcode.com/methods/new_teaser_TMZlD2J.jpg" alt="FPN Explained | Papers With Code"/>
<p>이 그림은 FPN을 검색하면 나오는 대표적인 그림이다. 이를 바탕으로 설명한다.</p>
<ul>
<li>
<p><strong>Featurized image pyramid</strong></p>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932665573559640134/unknown.png" alt="img"/>
<p>위 background에서 설명했던 내용처럼 Large Scale을 맞추기 위해서는 방법이 두 가지가 있다. 하나는 Window box를 늘리거나 또 하나는 Image size를 조정하는 것이다. <strong>Featurized image pyramid</strong>는 후자이다. 따라서 Image size를 변경해서 예측한다. 하지만 이는 자원소모가 크고 빠른 속도를 기대할 수 없다. 따라서 다른 방식이 나오기 시작한다. 해당 방법을 쓴 model은 <strong>Overfeat, HOG, SIFT</strong> 등이 있다.</p>
<img src="https://blog.kakaocdn.net/dn/mr4Lo/btqPe13oMhP/pK4EWkPsTbHELwJGcdMIT0/img.jpg" alt="img"/>
</li>
<li>
<p><strong>Single feature map</strong></p>
<p>하나의 image를 넣어서 사용하는 방법이다.  속도 측면에서는 좋지만 이전 방법보다는 정확도가 떨어진다는 단점이 있다. 가장 대표적으로 <strong>Yolov1</strong>이 있다.</p>
<img src="https://curt-park.github.io/images/yolo/Figure3.JPG" alt="img"/>
</li>
<li>
<p><strong>Pyramidal feature hierarchy</strong></p>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932670820235509810/unknown.png" alt="img"/>
<p>&quot;그러면 Large Scale을 적용하는 방법으로 Image 크기를 줄이는 법이 있는데 VGG같은 경우 일정 비율로 pooling 되니까 이를 stage마다 적용하는 방법이 있지 않을까?&quot; 에서 시작 했다고 생각한다. 가장 대표적인 모델은 <strong>SSD</strong>가 있다.</p>
<img src="https://d3i71xaburhd42.cloudfront.net/4e27fec1703408d524d6b7ed805cdb6cba6ca132/19-Figure3.1-1.png" alt="SSD-Sface : Single shot multibox detector for small faces | Semantic Scholar"/>
<p>근데 아까 <strong>High Resolution이면 low feature</strong>를 갖는다고 했고 <strong>Low Resolution이면 high feature</strong>를 갖는다고 했다.  forward 도중 predict를 진행하기에 cost 측면에서는 좋다. 하지만 높은 해상도에서는 좋은 feature들을 얻을 수 없다. 그렇기에 높은 해상도에서 관측 가능한 작은 객체들을 탐지하기 어려울 것이다.</p>
<p>그래서 여기서 하나 단어를 잡고 가면 <strong>Semantic</strong> Gap이다.</p>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932675643722838077/unknown.png" alt="img"/>
</li>
<li>
<p><strong>Feature Pyramid Network</strong></p>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932676070908502046/unknown.png" alt="img"/>
<p>그래서 나온게 FPN이다.  그러면 봐야하는 특징이 세 가지 있다.(Yolov3 모델로 설명할 것이다. 사실 완벽한 FPN은 아닌 것 같다.)</p>
<ul>
<li>
<p>bottom-up pathway</p>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932682821896667236/unknown.png" alt="img"/>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932682880541392977/unknown.png" alt="img"/>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932678088419061810/unknown.png" alt="img"/>
<p>해당 부분은 downsampling 과정이고 해상도를 낮추면서 feature level을 높이는 단계이다. FNP 논문에서는 ResNet을 기준으로 설명했다.</p>
</li>
<li>
<p>top-down pathway</p>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932678088419061810/unknown.png" alt="img"/>
<p>해당 부분은 upsampling 과정이다. 해상도를 높이는데 feature를 갖고 간다. 방법은 nn.Upsampling 공식 문서를 확인해도 되지만 간략히 설명하면 neareast는 아래와 같은 방식으로 동작한다.</p>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932680429964787712/unknown.png" alt="img"/>
</li>
<li>
<p>lateral connections</p>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932683196317970502/unknown.png" alt="img"/>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932678851274879056/unknown.png" alt="img"/>
<p>여기 부분이 yolov3와 다르다. FNP는 +를 한다. 하지만 yolov3는 depth기준 concat을 수행한다.</p>
<img src="https://cdn.discordapp.com/attachments/904717831940239403/932681564372992052/unknown.png" alt="img"/>
<p>해당 부분이 어떤 의미가 있는지는 좀 더 찾아보겠다.</p>
<p>본 논문에서는 아래와 같은 + 연산으로 1x1 conv에서 output 값을 맞춰준다. 따라서 같은 shape로 + 연산이 이루어진다.</p>
<img src="https://blog.kakaocdn.net/dn/0ela2/btqUsdFXuAe/zSFO8k1p1JIbMoz5vWi75k/img.png" alt="img"/>
</li>
</ul>
</li>
</ul>
<h2>Region Proposal Network(RPN)</h2>
<p>해당 내용은 <strong>Faster-RCNN</strong>에서 나온 내용이다. <strong>Yolov2</strong>에서 anchor는 위 논문을 보고 따왔다.</p>
<pre><code>3.1 Region Proposal Networks
A Region Proposal Network (RPN) takes an image
(of any size) as input and outputs a set of rectangular
object proposals, each with an objectness score.3 We
model this process with a fully convolutional network
[7], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN
object detection network [2], we assume that both nets
share a common set of convolutional layers. In our experiments, we investigate the Zeiler and Fergus model
[32] (ZF), which has 5 shareable convolutional layers
and the Simonyan and Zisserman model [3] (VGG-16),
which has 13 shareable convolutional layers.
To generate region proposals, we slide a small
network over the convolutional feature map output
by the last shared convolutional layer. This small
network takes as input an n × n spatial window of
the input convolutional feature map. Each sliding
window is mapped to a lower-dimensional feature
(256-d for ZF and 512-d for VGG, with ReLU [33]
following). This feature is fed into two sibling fullyconnected layers—a box-regression layer (reg) and a
box-classification layer (cls). We use n = 3 in this
paper, noting that the effective receptive field on the
input image is large (171 and 228 pixels for ZF and
VGG, respectively). This mini-network is illustrated
at a single position in Figure 3 (left). Note that because the mini-network operates in a sliding-window
fashion, the fully-connected layers are shared across
all spatial locations. This architecture is naturally implemented with an n×n convolutional layer followed
by two sibling 1 × 1 convolutional layers (for reg and
cls, respectively).
3.1.1 Anchors
At each sliding-window location, we simultaneously
predict multiple region proposals, where the number
of maximum possible proposals for each location is
denoted as k. So the reg layer has 4k outputs encoding
the coordinates of k boxes, and the cls layer outputs
2k scores that estimate probability of object or not
object for each proposal4
. The k proposals are parameterized relative to k reference boxes, which we call
anchors. An anchor is centered at the sliding window
in question, and is associated with a scale and aspect
ratio (Figure 3, left). By default we use 3 scales and
3 aspect ratios, yielding k = 9 anchors at each sliding
position. For a convolutional feature map of a size
W × H (typically ∼2,400), there are W Hk anchors in
total.
</code></pre></div><footer data-testid="footer" class="absolute bottom-0 w-full whitespace-nowrap bg-white p-4 px-2 dark:bg-gray-900 sm:p-6"><div class="container mx-auto"><div class="sm:flex sm:items-center sm:justify-between"><span class="text-sm text-gray-500 dark:text-gray-400 sm:text-center">© 2022 <a href="https://flowbite.com/" class="hover:underline">jeongdongha.me</a>. All Rights Reserved.</span><div class="mt-4 flex justify-center space-x-6 sm:mt-0"><a href="https://www.instagram.com/dhjeong4219/" class="text-gray-500 hover:text-gray-900 dark:hover:text-white"><svg class="h-5 w-5" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M12.315 2c2.43 0 2.784.013 3.808.06 1.064.049 1.791.218 2.427.465a4.902 4.902 0 011.772 1.153 4.902 4.902 0 011.153 1.772c.247.636.416 1.363.465 2.427.048 1.067.06 1.407.06 4.123v.08c0 2.643-.012 2.987-.06 4.043-.049 1.064-.218 1.791-.465 2.427a4.902 4.902 0 01-1.153 1.772 4.902 4.902 0 01-1.772 1.153c-.636.247-1.363.416-2.427.465-1.067.048-1.407.06-4.123.06h-.08c-2.643 0-2.987-.012-4.043-.06-1.064-.049-1.791-.218-2.427-.465a4.902 4.902 0 01-1.772-1.153 4.902 4.902 0 01-1.153-1.772c-.247-.636-.416-1.363-.465-2.427-.047-1.024-.06-1.379-.06-3.808v-.63c0-2.43.013-2.784.06-3.808.049-1.064.218-1.791.465-2.427a4.902 4.902 0 011.153-1.772A4.902 4.902 0 015.45 2.525c.636-.247 1.363-.416 2.427-.465C8.901 2.013 9.256 2 11.685 2h.63zm-.081 1.802h-.468c-2.456 0-2.784.011-3.807.058-.975.045-1.504.207-1.857.344-.467.182-.8.398-1.15.748-.35.35-.566.683-.748 1.15-.137.353-.3.882-.344 1.857-.047 1.023-.058 1.351-.058 3.807v.468c0 2.456.011 2.784.058 3.807.045.975.207 1.504.344 1.857.182.466.399.8.748 1.15.35.35.683.566 1.15.748.353.137.882.3 1.857.344 1.054.048 1.37.058 4.041.058h.08c2.597 0 2.917-.01 3.96-.058.976-.045 1.505-.207 1.858-.344.466-.182.8-.398 1.15-.748.35-.35.566-.683.748-1.15.137-.353.3-.882.344-1.857.048-1.055.058-1.37.058-4.041v-.08c0-2.597-.01-2.917-.058-3.96-.045-.976-.207-1.505-.344-1.858a3.097 3.097 0 00-.748-1.15 3.098 3.098 0 00-1.15-.748c-.353-.137-.882-.3-1.857-.344-1.023-.047-1.351-.058-3.807-.058zM12 6.865a5.135 5.135 0 110 10.27 5.135 5.135 0 010-10.27zm0 1.802a3.333 3.333 0 100 6.666 3.333 3.333 0 000-6.666zm5.338-3.205a1.2 1.2 0 110 2.4 1.2 1.2 0 010-2.4z" clip-rule="evenodd"></path></svg><span class="sr-only">Instagram page</span></a><a href="https://twitter.com/jdh4219" class="text-gray-500 hover:text-gray-900 dark:hover:text-white"><svg class="h-5 w-5" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.29 20.251c7.547 0 11.675-6.253 11.675-11.675 0-.178 0-.355-.012-.53A8.348 8.348 0 0022 5.92a8.19 8.19 0 01-2.357.646 4.118 4.118 0 001.804-2.27 8.224 8.224 0 01-2.605.996 4.107 4.107 0 00-6.993 3.743 11.65 11.65 0 01-8.457-4.287 4.106 4.106 0 001.27 5.477A4.072 4.072 0 012.8 9.713v.052a4.105 4.105 0 003.292 4.022 4.095 4.095 0 01-1.853.07 4.108 4.108 0 003.834 2.85A8.233 8.233 0 012 18.407a11.616 11.616 0 006.29 1.84"></path></svg><span class="sr-only">Twitter page</span></a><a href="https://github.com/ha4219" class="text-gray-500 hover:text-gray-900 dark:hover:text-white"><svg class="h-5 w-5" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd"></path></svg><span class="sr-only">GitHub account</span></a></div></div></div></footer></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script>
  
  
  if(true) {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  }
  if (typeof ga === "function") {
    ga('create', 'G-4XF51X6MBN', 'auto', {});
      
      
      
      
      
      }</script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/posts/posts/fpn/";/*]]>*/</script><!-- slice-start id="_gatsby-scripts-1" -->
          <script
            id="gatsby-chunk-mapping"
          >
            window.___chunkMapping="{\"app\":[\"/app-6afe5375e6bd201d9987.js\"],\"component---src-pages-404-tsx\":[\"/component---src-pages-404-tsx-86a87f9b4baad5cda519.js\"],\"component---src-pages-about-tsx\":[\"/component---src-pages-about-tsx-f78ad95acf27b7c05af4.js\"],\"component---src-pages-index-tsx\":[\"/component---src-pages-index-tsx-9fde7b556601256fcd6f.js\"],\"component---src-pages-posts-mdx-fields-slug-tsx-content-file-path-users-jeongdongha-code-toy-ha-4219-github-io-contents-posts-fpn-mdx\":[\"/component---src-pages-posts-mdx-fields-slug-tsx-content-file-path-users-jeongdongha-code-toy-ha-4219-github-io-contents-posts-fpn-mdx-c5e1c964664c492626ca.js\"],\"component---src-pages-posts-mdx-fields-slug-tsx-content-file-path-users-jeongdongha-code-toy-ha-4219-github-io-contents-posts-test-algorithm-mdx\":[\"/component---src-pages-posts-mdx-fields-slug-tsx-content-file-path-users-jeongdongha-code-toy-ha-4219-github-io-contents-posts-test-algorithm-mdx-e5053c1e11296aa17cc6.js\"],\"component---src-pages-posts-mdx-fields-slug-tsx-content-file-path-users-jeongdongha-code-toy-ha-4219-github-io-contents-posts-test-mdx\":[\"/component---src-pages-posts-mdx-fields-slug-tsx-content-file-path-users-jeongdongha-code-toy-ha-4219-github-io-contents-posts-test-mdx-899b77c04ef1b4ffcdf1.js\"],\"component---src-pages-projects-tsx\":[\"/component---src-pages-projects-tsx-ba777ef7510d819ce137.js\"],\"component---src-templates-category-tsx\":[\"/component---src-templates-category-tsx-380a4d3c168b506b72fb.js\"]}";
          </script>
        <script>window.___webpackCompilationHash="4281fc0902b0eef0d0fc";</script><script src="/webpack-runtime-8e893383fbaf47cf2bcd.js" async></script><script src="/framework-ee6843b615fb900f2a1a.js" async></script><script src="/app-6afe5375e6bd201d9987.js" async></script><!-- slice-end id="_gatsby-scripts-1" --></body></html>