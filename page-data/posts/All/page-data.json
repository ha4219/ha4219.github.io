{"componentChunkName":"component---src-templates-category-tsx","path":"/posts/All/","result":{"pageContext":{"currentCategory":"All","categories":["All","daily","review","devops","vision","algorithm"],"edges":[{"node":{"frontmatter":{"title":"23.01.24","date":"2023-01-24T00:00:00.000Z","moment":"15 hours ago","thumbnail":null,"tags":["daily"],"category":"daily"},"excerpt":"설 연휴 마지막날 일기 생각정리\n\n개인적인 이슈가 크게 터졌다. 원래 연휴 때 낮에 책 읽고 밤에 친구들이랑 게임하려고 했는데 모든 것이 엉망이 됐다. 이슈에 관해서 생각을 진짜…","id":"369b64a2-6f1b-5575-8586-bac3ea97fc78","body":"\n# 23.01.24\n\n## 설 연휴 마지막날 일기\n\n### 생각정리\n\n 개인적인 이슈가 크게 터졌다. 원래 연휴 때 낮에 책 읽고 밤에 친구들이랑 게임하려고 했는데 모든 것이 엉망이 됐다. 이슈에 관해서 생각을 진짜 많이 했는데 나중에 내 스스로 객관화를 한 후 포스팅해야 겠다.\n\n1. 금요일, 랩 미팅\\\n    기존 모든 이미지를 사용하는 것을 대신해 **지역 별 clustering**을 통해 **동적**으로 데이터셋을 불러와 학습\n    이유: 기존 이미지는 드론으로 여러 장소를 연속으로 측정함. 따라서 한 장소에 오랫동안 데이터 수집을 했을 경우 그 장소에 대해서만 데이터 셋이 많음. 그렇기에 비슷한 데이터를 반복적으로 사용하는 것은 학습에 좋지 않은 결과를 띌 것이다.\n    결과적으로 지역을 clustering할 방법에 대해 찾는다면 가능하다. 이는 내일 찾아봐야 겠다. (분류인가 clustering인가, 분류라면 라벨링을 해야하는 것인가)\n    이미지를 지역별로 분류할 수 있다면 [Dataset](https://pytorch.org/vision/stable/datasets.html)에서 동적으로 이미지를 넘겨줄 수 있을 것이다.(병렬적인 부분은 고려하지 않았다) → `Datasets.__len` = 지역 수?\n2. [js deep dive](http://www.yes24.com/Product/Goods/92742567) 읽기\n    700p까지 읽었으니까 내일 근로 때 적당히 읽으면 될 것 같다.\n3. [운영체제](http://www.yes24.com/Product/Goods/111378840) 책 읽기\n    책 내용이 너무 쉽다. 먼저 js 책 다 읽고 이 책은 포스팅할 거 정리하면서 읽어야겠다.\n4. 토익 문제 풀기\n    진짜 공부해야하는데…\n5. 연구실 짐 옮기기\n    내일 맥북이랑 충전기들만 옮겨서 작업하고 나머지는 천천히 해야될 것 같다.\n\n\n---\n\n[https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics)\n\n 좋은 건지 나쁜 건지 모르겠지만 `yolov8`이 나왔다. 처음 프로젝트에 들어갔을 때 `yolov5`가 나와서 이것을 현 프로젝트에 적용시켰는데 해당 저자가 개선된 버전을 올려줬다. 역시나 paper는 없다. 변경 내용은 **모델 개선**(사실 이게 제일 중요한 거 같은데 paper가 없어서 분석이 어려울 것 같다), **패키지 배포** 등이 있는데 지금 가장 중요한 **edgetpu** 모델로 변환할 수 없다.(torch based model) 그래서 당장은 적용할 수 없겠지만 관심을 갖을 필요는 있을 것 같다.\n\n---\n\n[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n\n Transformer 모델에서 Decoder 부분을 사용해 만들었다는 GPT, 논문을 직접 읽은 것도 아니지만 [Andrej Karpathy](https://karpathy.ai/)가 이를 바닥부터 만드는 영상이다. 재밌을 것 같아서 메모한다.\n\n---\n\n### 하고 싶은 것\n\n- [Goodbye, useEffect](https://www.youtube.com/watch?v=bGzanfKVFeU&t=1113s), review\n- javascript deep dive, review\n- 25일 유퀴즈 곽튜브편 보기\n- 주식 글 정리 및 계획\n- blog 마무리\n- discord bot, game 구현\n","fields":{"timeToRead":{"minutes":3.76},"slug":"/posts/2023-01-24/23-01-24-52e13892ffdd418380e14279a9d6b5a5/"},"internal":{"contentFilePath":"/Users/jeongdongha/code/toy/ha4219.github.io/contents/posts/2023-01-24/23-01-24-52e13892ffdd418380e14279a9d6b5a5.mdx"}},"next":{"fields":{"slug":"/posts/2023-01-04/22-reivew/"}},"previous":null},{"node":{"frontmatter":{"title":"22년 회고록","date":"2023-01-04T00:00:00.000Z","moment":"21 days ago","thumbnail":null,"tags":["review"],"category":"review"},"excerpt":"lab 1. 논문 읽기\n\n물론 더 많이 읽었지만 가장 기억에 남는 두 개를 뽑았다.\n\nFeature Pyramid Networks for Object Detection (22…","id":"5197e1d5-6e6e-579b-baea-0b0c1243807d","body":"\n## lab\n\n---\n\n![lab-photo](22-review/Untitled.png)\n\n### 1. 논문 읽기\n\n물론 더 많이 읽었지만 가장 기억에 남는 두 개를 뽑았다.\n\n- [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144) (22.01.10)\n    - 객체 탐지에 대해 공부하면서 읽었고 scale이 다른 객체를 탐지할 때 어떻게 효과적으로 추출할지 모델 구조를 제안한 방법이다. 막연히 YOLO를 공부하면서 읽은 것들 중 성능과 무관하게 가장 신박했다.\n- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (22.12.01)\n    - 최근 트렌드를 따라가기 위해 공부한 내용으로 자연어 처리에 대한 지식이 부족하다보니 처음에 공부하기가 힘들었다.\n        [](https://www.coupang.com/vp/products/213557542?itemId=648143817&vendorItemId=72056821737&src=1042503&spec=10304025&addtag=400&ctag=213557542&lptag=10304025I648143817V72056821737&itime=20230103201815&pageType=PRODUCT&pageValue=213557542&wPcid=16692759457882830936333&wRef=&wTime=20230103201815&redirect=landing&gclid=CjwKCAiAwc-dBhA7EiwAxPRylOEdV2_s6SfHK7VWrx3EPEIPuJUxdOfD21_z-G8n2hQ-QAnSNbhY0RoCynYQAvD_BwE&campaignid=17373571394&adgroupid=&isAddedCart=)\n        개인적으로 [물고기책 1권](http://www.yes24.com/Product/Goods/34970929)을 좋아해서 2권도 사고 군대에서 한 번 대충 읽고 버려뒀는데 위 논문을 읽기 위해 다시 공부했다.(물고기 책이 왜 좋은지 알 수 있었다…)\n    - **Transformer**의 기초가 되는 Encoder와 Decoder를 배울 수 있었고 **Attention** 기법을 어떻게 활용하는 지를 알 수 있었다. 개인적으로 이를 활용해 Object Detection을 사용하는 아이디어가 있는 데 이를 방학동안 실험해 보려고 한다.\n- 이를 제외하고도 거의 매 주(?) 세미나 발표를 통해 `yolo_v5`, `yolo_v7`, `object tracking` 공부했다. 특히 `yolo_v5` 코드를 매우 잘 짰다고 생각해서 repo를 계속 새로 파면서 처음부터 다시 짜보는 거를 반복한 것 같다.\n\n### 2. Keepincoin (22.02.15 ~ 09.05)\n\n 주 개발 기간은 위와 같지만 계속 유지 및 보수를 하고 있다.\n\n![keepincoin_summarization](22-review/Untitled-1.png)\n\n\n- torch==1.11.0\n- fastapi==0.78.0\n\n[https://github.com/ha4219/keepincoin](https://github.com/ha4219/keepincoin)\n\n 서버 프로젝트로 이미지를 **Client** 에서 받아 **Coin** 으로 생성하는 방법이다. 사실 핵심 기술인 **coinGenerator** 부분은 교수님 코드를 받아서 이를 **linux** 서버에서 사용할 수 있게 `.so` 파일로 변환하고 서버에 올려 아래와 같은 **sequential한**(?) 로직을 수행한다. 개인적으로 배운 내용은 **Docker**라고 생각한다. (**[ASGI](https://asgi.readthedocs.io/en/latest/)** 때문에 **fastapi**를 선택했지만 찾아보니 **Flask**도 지원 했기에 여기에 쓰지 않았다)\n\n 처음 프로젝트를 친구와 함께 **Flask**로 진행했다. 하지만 지속적인 수정과 배포로 진짜 머리가 아팠다. 환경이 안 맞춰져 있어서 개발 환경에서 개발을 못하고 서버에서 vi로 코드를 수정하고 돌리고 에러 뜨면 될 때까지 수정하고 돌리는 작업을 반복했다. 그래서 교수님 카톡이 오면 좀 무서웠다.\n\n 따라서 위와 같은 이유로 1학기가 끝나자마자 **Docker**로 배포해야 겠다는 생각으로 새로 **repo**를 파서 진행했다. 메인 앱을 **Docker**로 publish하고 서버에서 최신 이미지를 **hub**에서 받아와 **nginx**와 함께 **Docker-compose**로 배포했다. 덕분에 공부도 많이 됐고 지금은 쉽게쉽게 배포하고 있다.\n\n### 3. [연구실 사이트](https://cgai-lab.github.io/) (22.08.24 ~ 22.09.07)\n\n![lab-site_summarization](22-review/Untitled-2.png)\n\n[https://github.com/cgai-lab/cgai-lab.github.io](https://github.com/cgai-lab/cgai-lab.github.io)\n\n- gatsby\n- react\n- mui\n- [Contentful](https://www.contentful.com/) (Content Management System)\n\n 교수님 연구실 사이트가 없어서 계속 만들어야지 생각하고 있다가 2학기 개강 직전에 빠르게 만들었다. 하나의 문제가 있다면 **SEO** 부분이다. 디자인이 없어서 다른 연구실 사이트들을 참고하면서 개발했다. 그래서 피드백을 받고 있는 중인데 교수님이 바쁘셔서 아직도 수정할 부분이 많지만 정지 상태이다. 겨울 방학 때 마무리 짓고 싶다. 배운 내용으로는 `CI/CD` 이다. **Gatsby** 특성상 필요조건이라고 생각한다. 이전에 **Docker**를 배운 것이 매우 큰 도움이 됐다. 처음에는 **github actions**을 썼는데 **CMS** 때문에 **circleci**로 변경했다.\n\n 개인적으로 깔끔하지 못한 부분이 하나 있는데 학교 도메인을 달기 위해 ftp로 특정 주소에 파일을 올려야 하는데 학교 주소가 아니면 접근이 불가했다. 그래서 공인 아이피로 된 랩 서버에 trigger를 두긴 했는데 이를 해결 할 방법이 분명히 있을 것 같다. **SEO** 부분을 하면서 수정할 계획이다.\n\n## 학교 수업\n\n---\n\n### 1. Software engineering (1학기)\n\n\n![Tetris-demo](22-review/Untitled-3.png)\n\n[https://github.com/SE-Team6/SE_Project_Tetris_Java_Swing](https://github.com/SE-Team6/SE_Project_Tetris_Java_Swing)\n\n 학교 친구들이랑 같이 진행한 팀 프로젝트로 수업 시간에 배운 내용을 이용해 테트리스를 구현하는 프로젝트였다. 기능적으로는 **아이템전**, **멀티모드**, 설정에서 **색약**과 **화면 크기** 등 실제 게임을 배포한다는 생각으로 프로젝트를 진행했다. 개인적으로 어려웠던 부분은 **배포**와 **문자열** 부분이다. 먼저 배포는 팀원들도 공감할 것이 하루만에 될 거라고 생각했는데 생각보다 잘 안돼서 중간 발표 전날에 밤을 샐 뻔 했다. 다행히 다른 잘 하는 친구가 찾아서 해줬지만 생각보다 어려웠다. 처음에는 **맥**으로 배포하려고 했지만 모험인 것 같아서 **윈도우**로 변경했다. (다른 `.pkg` 파일에 대해 감사함을 느낀다) 문자열은  위 데모 사진을 보면 블록들 위치가 예쁘게 맞아 떨어진다. 해당 화면은 문자열로 채워져 있다. 그래서 문자열 크기가 안맞아서 모양이 뒤틀린 것처럼 보이고 그랬다. 처음 맥에서는 예쁘게 잘 나왔는데 또 윈도우 넘어가니까 화면이 뒤틀렸다. ㅋㅋㅋㅋ. 어찌저찌 맞는 문자를 찾아서 해결했다…\n\n### 2. National Language Processing (1학기)\n\n![emogenius-summarization](22-review/Untitled-4.png)\n\n![emogenius-demo](https://github.com/ha4219/emogenius/raw/dev/assets/readme/run.gif?raw=true)\n\n[https://github.com/ha4219/emogenius](https://github.com/ha4219/emogenius)\n\n 개인적으로 애정을 갖고 했던 프로젝트로 한국어 문장의 기분을 분류해 이에 맞는 **emoji**를 추천하는 프로젝트다. 프로젝트 주제를 고민하고 있을 때 다른 친구가 이 주제를 던져줬고 이를 낼름 받아먹서 혼자 진행했다. 실제 **chrome-extension**에 배포도 해서 잘 사용했지만 서버 요금 때문에 내렸다.(torch를 사용하기 위해 프리티어를 사용할 수 없었다) 이 때까지만 해도 내가 **NLP**를 공부할 지 몰랐다.\n\n 교수님이 칭찬을 많이 해주셨다. 감사합니다~\n\n### 3. Advanced Web Programming (2학기)\n\n![MBTIWithUs-logo](22-review/Untitled-5.png)\n\n[MBTI With Us](https://github.com/MBTIWithUs)\n\n 2학기 때 시간이 많이 남아 공들인 프로젝트이다. MBTI 검사가 **자가보고형**이기 때문에 신뢰가 없다는 글을 읽고 이 프로젝트를 생각했다. **Front-End**를 맡았고  `cra` 로 **react** 프로젝트를 만들어 **다크 모드**, **반응형 웹**, **무한 스크롤** 등 하고 싶은 것을 다 해봤다. 내 생각에는 실제 배포해도 큰 문제가 없지만 서버하는 친구가 **DB**와 **웹서버**를 분리하지 않아서 **AWS 무료 버전**에서 안돌아 가는 것 같다. 그래서 지금은 내린 상태이지만 그래도 같이 고생했기에 진짜 고마웠다.(기획이 없기도 했지만 내가 의사소통에 문제가 있는 것 같다) 결과는 당연히 좋았다!\n\n## 공모전\n\n---\n\n### 1. kube-form (22.07.17 ~ 08.10)\n\n![kube-form-logo](22-review/Untitled-6.png)\n\n![kube-form-photo](22-review/Untitled-7.png)\n\n[kube-form](https://github.com/kube-form)\n\n 진짜 잘하는 형이랑 진행한 프로젝트 여기서도 **Front-End**를 맡았으며 같이 하는 형 한 분이 있었다. **AWS**의 **EKS** 을 다루는 **Dashboard**를 만드는 다시 생각해봐도 실제 어려운 부분은 백엔드 로직이였던 것 같다. 기능적인 부분은 **Drag&Drop, IAM 암호화** 등이 있었던 것 같다. 개인적으로 **hook**과 **context**로 유저 관리 같은 부분을 많이 공부했다.\n\n 위 사진에서는 내가 없는데 학회 일정이랑 겹쳐서 교수님 따라 학회가서 맛있는 거 먹었다.\n\n## 개인\n\n---\n\n### 1. 운동 (22.09 ~)\n\n 맨날 해야지 생각하지만 게을러서 못하는 운동을 친구랑 같이 헬스장에 등록하면서 시작했다. 당연히 개초보여서 자극은 잡지도 못하고 보이는 기구 반복만 했다. 그러다가 2학기 중간 끝나고 PT를 끊었는데 확실히 몇 개 배운 것 같다. 최근에 다 끝났는데 이제 배운 것만 반복하려고 한다. 그리고 생각보다 근육이 붙는다는 것이 신기하다. 지금까지 gpu 4개 달아놨는데 single gpu로 학습 돌린 것 같다.\n\n### 2. 책 (22.08 ~)\n\n 원래 책을 읽으면 거의 지식이 필요해서 전공 책만 읽었는데 친구가 추천해준 **화신귀환**을 읽으면서 다른 책들도 많이 찾아보고 있다. (근데 화산귀환 해남 가기 전까지는 재밌긴 하다) 처음에 밀리로 읽다가 지금은 책을 빌리거나 전자 도서관에서 빌리는 데 확실히 구매보다는 더 좋은 것 같다.\n\n### 3. 온라인 강의\n\n 개인적으로 연초에 **인프런**이나 **유튜브** 이런 강의들을 몇 개 봤었는데 **udemy**에 정말 좋은 강의들이 많은 것 같다. 특히 **FE**에서 **test code 작성**을 어떻게 해야하는 지 이게 정말 고민이 많았고 의문이였는데 강의를 보고 이해할 수 있었다. 새로 블로그를 개설하는데 잘은 못하더라도 조금씩 적용해 보고 있다.\n\n### 4. 근로 (21.12 ~)\n\n 사실 금방 그만둘 줄 알았는데 지금까지 계속하고 있다. 딱 21년에 인턴 끝나고 했는데 벌써 1년이다. 별로 일을 안시켜서 위에 책을 근로 시간에 읽고 있다… 그냥 맨날 지각해서 선생님들께 죄송할 따름이다…\n\n## 총평\n\n---\n\n 글을 쓰면서 1년을 되짚어보면 학기중에는 **수업** 아니면 **근로**로 학교 6시까지 시간을 보내고 6시 이후에는 연구실에서 코딩을 했고 여름 방학 때는 `'화산귀환'|'coding'` 이였던 것 같다. 이번 블로그를 개설하면서 처음으로 회고록을 써보는 데 생각보다 1년을 되돌아 본다는 점이 매우 좋은 것 같다.\n\n 그렇다고 “내가 더 잘해졌다?”는 아닌 것 같다. 최근 면접을 보고 와서 느낀 생각이 아직도 되새기고 있다. 간단히 말하면 **docs**를 읽는 것은 모두가 할 수 있는 능력이라고 생각한다. 프로젝트를 진행하면서 많은 **docs**를 읽어가며 세팅하고 개발하고 어떻게 보면 누군가가 만들어둔 **module**을 내가 쓴 거에 지나지 않는다. (react, nextjs, fastapi, docker 등) 그게 큰 의미가 있을까? 3학년 때 이것을 깨닫게 된게 매우 후회스럽지만 일단 많은 에러를 경험하면서 배웠기에 이를 활용할 수 있다는 장점 뿐인 것 같다. 그래서 그 이후 `모던 자바스크립트 Deep Dive` 를 읽고 있는데 생각보다 모르는 부분이 많았다.(언어의 원초적인 매력을 배울 수 있었고 이 내용 또한 블로그에 따로 작성할 계획이다)\n\n 22년에는 알고리즘이 빠졌는데 이거는 내가 게을러서 그렇다… 23년에는 잡아야겠다.\n\n## 목표\n\n---\n\n1. 영어 점수 획득\\\n    영어를 잘 못하기도 하지만 영어 점수가 없다는 것도 크다. 공부해서 받아야 내가 무엇을 할 지를 판가름할 수 있을 것 같다.\n2. 책\\\n    재밌는 책 있으면 이것도 리뷰해야지\n3. 알고리즘\\\n    군대에서는 매일 한 문제씩 풀었는데 전역하니까 못하겠다. 취업을 위해서라도 다시 잡아야 한다… **codeforces** **virtual**로 돌리고 있다.\n4. lab\\\n    영어 성적에 따라 갈리겠지만 일단 1학기 때까지는 있을 생각이다. 그때까지 **Transformer**를 보면서 생각해둔 모델을 한 번 실험해야 한다.\n5. Front-End\\\n    **프레임 워크**를 배운다기 보다는 좀 더 기초로 돌아가 다시 공부해보려고 한다. 하면서 최적화나 아니면 새로운 모듈을 만들거나 해야겠다.","fields":{"timeToRead":{"minutes":15.84},"slug":"/posts/2023-01-04/22-reivew/"},"internal":{"contentFilePath":"/Users/jeongdongha/code/toy/ha4219.github.io/contents/posts/2023-01-04/22-reivew.mdx"}},"next":{"fields":{"slug":"/posts/2022-12-20/cicd-lecture-memo/"}},"previous":{"fields":{"slug":"/posts/2023-01-24/23-01-24-52e13892ffdd418380e14279a9d6b5a5/"}}},{"node":{"frontmatter":{"title":"CICD 강의 메모","date":"2022-12-20T00:00:00.000Z","moment":"a month ago","thumbnail":null,"tags":["devops"],"category":"devops"},"excerpt":"git-flowdocker Namespace\npidnetworkIPCMountUNIX Time-sharing\nCGroup\ncpumemoryblock I/O…","id":"8814ed8f-884b-58ce-b92c-4e07245fb787","body":"\n# CI/CD\n\n- git-flow\n- docker\n    - Namespace\n        - pid\n        - network\n        - IPC\n        - Mount\n        - UNIX Time-sharing\n    - CGroup\n        - cpu\n        - memory\n        - block I/O\n        - network\n    - union file system\n    - `-it`\n        For interactive processes (like a shell), you must use `-i -t` together in order to allocate a tty for the container process. `-i -t` is often written `-it` as you’ll see in later examples. Specifying `-t` is forbidden when the client is receiving its standard input from a pipe, as in:\n        ```bash\n        echo test | docker run -i busybox cat\n        ```\n- docker file 작성\n    - CMD & ENTRYPOINT\n- linux\n    - `tail -f` : tail follow\n    - `history` → `!{number}`\n    - layered file system\n        - `from ubuntu18`\n    - history 남기지 않기\n        - `공백` command\n- python\n    - `fire`\n        - support CLI\n- Jenkins\n    - kubernetes\n        - argo cd\n    - RBAC\n        - roll based access control\n","fields":{"timeToRead":{"minutes":0.735},"slug":"/posts/2022-12-20/cicd-lecture-memo/"},"internal":{"contentFilePath":"/Users/jeongdongha/code/toy/ha4219.github.io/contents/posts/2022-12-20/cicd-lecture-memo.mdx"}},"next":{"fields":{"slug":"/posts/2022-10-01/positional-embedding/"}},"previous":{"fields":{"slug":"/posts/2023-01-04/22-reivew/"}}},{"node":{"frontmatter":{"title":"Positional Encoding(Embedding)이 필요한 이유","date":"2022-10-01T00:00:00.000Z","moment":"4 months ago","thumbnail":null,"tags":["vision","nlp"],"category":"vision"},"excerpt":"Since our model contains no recurrence and no convolution, in order for the model to make use of…","id":"30f06b86-3005-5315-bc2e-a6a4893b018d","body":"\n> Since our model contains **no recurrence and no convolution**, in order for **the model to make use of the order of the sequence**, we must inject some information about the relative or absolute position of the tokens in the sequence.\n> \n\n> RNN과 LSTM과 다르게 트랜스포머는 **입력 순서가 단어 순서에 대한 정보를 보장하지 않는다.** 다시 말하면, 트랜스포머의 경우 시퀀스가 한번에 병렬로 입력되기에 단어 순서에 대한 정보가 사라진다. 따라서 단어 위치 정보를 별도로 넣어줘야 한다.\n> \n\n- Positional Encoding, Embedding 둘 다 입력 위치에 대한 값을 추가시키기 위한 목적이기에 아래에서는 PE로 통일\n\n## 문제 제기\n\n![Untitled](pe/Untitled.png)\n\n 위와 같이 **transformer** 입력이 주어질 것이다. 이때 width는 embedding 차원, height은 vocab_size가 될 것이다. 처음 생각은 위와 같은 데이터가 들어온다면 각 행은 단어에 관한 정보를 담고 있고 **행의 순서는 곧 단어의 순서를 의미하기 때문에 PE가 굳이 필요한가**라는 생각이 있었다.\n\n## 문제 해결\n\n### 데이터셋\n\n이에 대해 간단한 데이터셋으로 설명하겠다.\n\n```python\nMAX = 1000\n\ndata = []\n\nfor i in range(MAX):\n    for j in range(MAX):\n        data.apend(f'{i}+{j}' + f'={i+j}' + '\\n')\n        data.apend(f'{i}-{j}' + f'={i-j}' + '\\n')\n\nwith open('dataset/number_data.txt', 'w') as f:\n    f.writelines(data)\n```\n\n위 코드로 2,000,000 개의 데이터를 생성했다. 덧셈과 뺄셈식을 넣었을 때 결과를 **transformer** 모델이 학습해 이를 유추하는 모델이다. (학습 모델 및 데이터 전처리 과정 코드 생략)\n\n### Attention\n\n$$Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt d_k}\\right)V$$\n\nattention이 이를 잘 관찰하는가가 문제이다. 따라서 입력 “21+211=232”에 대한 $softmax\\left(\\frac{QK^T}{\\sqrt d_k}\\right)$ 값(가중치)을 시각화해 보겠다.\n\n- y axis: Query, x axis: Key\n\n1. PE 적용\n    1. Multi-Head Attention in Decoder(Query: Decoder output, Key: Encoder output)\n        ![Untitled](pe/Untitled-1.png)\n        빨간 Box를 확인해보겠다.\n        최종적으로 이는 설명하기 어렵다. 아래 다른 입력에 대해서는 설명하기 쉽게 가중치가 출력되지만 위 입력에 대해서는 이해하기 힘든 결과가 나왔다. 하지만 뒤에 나오는 결과가 의미가 있기에 이를 묵시하고 넘어간다.\n        ![Untitled](pe/Untitled-2.png)\n    2. Multi-Head Attention in Encoder(Query, Key: Encoder output)\n        ![Untitled](pe/Untitled-3.png)\n        ![Untitled](pe/Untitled-4.png)\n        두 번째 그림은 첫 번째 그래프에서 빨간 Box를 확대시킨 것이다.\n        이 또한 모든 행의 대해 설명하기는 어렵지만 대체로 이해하기 쉬운 결과를 내고 있다. 하지만 모든 Multi-Head에서 관측된 가중치 값에서 공통적인 내용이 있는데 이는 **십의 자리 “1”과 일의 자리 “1”을 그리고 십의 자리 “2”와 백의 자리 “2”를 구분한다**는 것이다. 만약 PE가 없으면 가능할까 아래 표를 더 확인해보자.\n2. PE 미적용\n    1. Multi-Head Attention in Decoder(Query: Decoder output, Key: Encoder output)\n        ![Untitled](pe/Untitled-5.png)\n        PE를 적용했을 때와 마찬가지로 이 또한 해석하기 어려운 결과를 보였다.\n    2. Multi-Head Attention in Encoder(Query, Key: Encoder output)\n        ![Untitled](pe/Untitled-6.png)\n        ![Untitled](pe/Untitled-7.png)\n        위 또한 설명하기 어려운 결과를 갖고 있지만 8개의 Multi-Head에서 보이는 공통적인 특징은 **십의 자리 “1”과 일의 자리 “1”을 그리고 십의 자리 “2”와 백의 자리 “2”를 구분하지 못하고 갖은 가중치를 갖는다.** 그러면 왜 구분하지 못할까?\n\n<Callout>\n💡 위 내용을 요약하면 **PE**를 제거한 Encoder 부분의 Self Attention에서 같은 문자열이 들어오면 **자리수를 구분하지 못하고 같은 가중치를 뽑아낸다**는 문제가 있다.\n</Callout>\n\n### 행렬 계산\n\n이는 $softmax\\left(\\frac{QK^T}{\\sqrt d_k}\\right)$를 계산하는 과정에서 이해할 수 있다.\n\n![Untitled](pe/Untitled-8.png)\n\n 먼저 **Query, Key, Value**는 입력 $x$에 대해 가중치 **W**를 이용해 생성된다. 처음 Encoder에서 생각해보면 $x$의 각 행은 단어를 의미할 것이고 행렬곱 연산을 적용한 후 나온 matrix의 각 행도 $x$의 행 값에 영향이 많을 것이다. 입력이 **“12+1”**이라면 행렬곱으로 생성된 $Query, Key$를 위와 같다고 생각할 수 있다.\n\n $X$의 십의 자리 “1”과 일의 자리 “1”은 같은 값을 갖을 것이다.(Word Embedding의 결과) 따라서 각각의 $Query, Key$ 행렬에서 십의 자리 “1”과 일의 자리 “1”은 같은 값을 갖을 것이다.\n\n![Untitled](pe/Untitled-9.png)\n\n![Untitled](pe/Untitled-10.png)\n\n이후 $energy$값을 구해도 십의 자리 “1”과 일의 자리 “1” 두 행의 값은 동일하다라는 것을 생각할 수 있고 이후 연산도 동일하다.\n\n결론적으로 나온 가중치 값은 Attention mechanism에서 매우 중요한 $Value$에 대해 어떤 단어를 집중할지를 보여줄 텐데 가중치 $\\alpha$에서 이미 이 두 개의 정보를 구분할 수 없기에 이후 나오는 output은 이에 대해 모호한 결과로 나올 것이다. **따라서 위치 정보가 없다면 학습하는 데 영향을 줄 수 있다.**\n\n위 데이터셋은 덧셈과 뺄셈에 관한 데이터셋이지만 이를 확장시켜 문장 번역 task에서도 한 문장안에 똑같은 단어가 주어졌을 때 이를 구분할 수 없을 것이다.\n","fields":{"timeToRead":{"minutes":6.64},"slug":"/posts/2022-10-01/positional-embedding/"},"internal":{"contentFilePath":"/Users/jeongdongha/code/toy/ha4219.github.io/contents/posts/2022-10-01/positional-embedding.mdx"}},"next":{"fields":{"slug":"/posts/2022-08-01/hungarian-algorithm/"}},"previous":{"fields":{"slug":"/posts/2022-12-20/cicd-lecture-memo/"}}},{"node":{"frontmatter":{"title":"Hungarian Algorithm","date":"2022-08-01T00:00:00.000Z","moment":"6 months ago","thumbnail":null,"tags":["algorithm"],"category":"algorithm"},"excerpt":"아래 글 참조 Assignment Problem and Hungarian Algorithm\n\nProblem Statement\n\n가장 대표적으로 NN…","id":"cd85f833-f1fd-5414-a646-2d322e70f5bc","body":"\n\n\n# Hungarian Algorithm\n\n> 아래 글 참조\n> \n> \n> [Assignment Problem and Hungarian Algorithm](https://www.topcoder.com/thrive/articles/Assignment%20Problem%20and%20Hungarian%20Algorithm)\n> \n\n<Callout>\n💡 call **out** test\n</Callout>\n\n## Problem Statement\n\n 가장 대표적으로 $N$명의 사람에게 $N$개의 일을 담당하는 문제를 생각해보자. 각 사람은 1개의 일을 담당할 수 있으며 이에 대한 $cost$가 발생한다. 그러면 기업 입장에서 목표는 이 비용을 최소화하며 $N$개의 일을 배치해야 한다. \n\n 아래는 위 설명을 식으로 설명한 것이다.\n\n$\\{c_{ij}\\}_{N\\times N}$: cost matrix, where $c_{ij}$: cost of $i$ to perform  job $j$.\n\n$\\{x_{ij}\\}_{N\\times N}$: resulting binary matrix, where $x_{ij} = 1$ if and only if $i_{th}$ worker is assigned to $j{th}$ job.\n\n$\\sum_{i=1}^{N}{x_{ij}} = 1$, $\\forall i\\in \\overline{1, N}$: one worker to one job assignment.\n\n$\\sum_{i=1}^{N}{x_{ij}} = 1$, $\\forall j\\in \\overline{1, N}$: one job to one worker assignment.\n\n$\\sum_{i=1}^N{\\sum_{j=1}^N{c_{ij}x_{ij}}} \\rightarrow min$: total cost function.\n\n우리는 이 문제를 그래프 문제로 바꿔 생각할 수 있다. $N$명의 사람에게 $N$개의 일에 대한 $cost$가 주어졌다고 생각하면 각 $N$명의 사람에게 $N$개의 일이 간선으로 연결되어 있어 총 $N*N$개의 간선이 연결된 그래프로 표현할 수 있다. 아래 예시를 보자.\n\n![Untitled](hungarian-algorithm/Untitled.png)\n\n## General Description Of The Algorithm\n\n이러한 문제를 할당문제라고 한다.\n\n이 문제를 foolish하게 해결하면 $O(n!)$이면 해결 할 수 있다. bfs, dfs로 순열을 찾고 이에 대한 cost를 계산해 최소값을 찾는다.\n\n쉽게 할 수 있지만 문제를 해결하기에 적합한 시간복잡도는 아니다.\n\n효율적인 방법의 알고리즘을 보여줄 텐데 하나는 쉽고 빠르게 $O(n^4)$이고 다른 하나는 구현이 복잡하지만 $O(n^3)$이다.\n\n## $O(n^4)$ Algorithm Explanation\n\n이분 그래프로 이 문제를 다루겠다. \n\n### Step 0)\n\n1. For each vertex from left part(workers) find the minimal outgoing edge and subtract its weight from all weights connected with this vertex. This will introduce 0-weight edges (at least one).\n2. Apply the same procedure for the vertices in the right part (jobs).\n\n![Untitled](hungarian-algorithm/Untitled-1.png)\n\n실제 이 순서는 필요하지 않지만 main cycle 수를 줄일 수 있다.\n\n### Step 1)\n\n1. Find the maximum matching using only 0-weight edges (for this purpose you can use max-flow algorithm, augmenting path algorithm, etc.).\n2. If it is perfect, then the problem is solved. Otherwise find the minimum vertex cover $V$(for the subgraph with 0-weight edges only), the best way to do this is to use [Köning’s graph theorem](http://www.topcoder.com/tc?module=LinkTracking&link=http://en.wikipedia.org/wiki/K%C3%B6nig%27s_theorem_(graph_theory)#proof&refer=hungarianAlgorithm).\n\n![Untitled](hungarian-algorithm/Untitled-2.png)\n\n### Step 2)\n\nLet $\\triangle min_{i \\notin V, j \\notin V}{c_{ij}}$ and adjust the weights using the following rule:\n\n![Untitled](hungarian-algorithm/Untitled-3.png)\n\n![Untitled](hungarian-algorithm/Untitled-4.png)\n\n### Step 3)\n\nRepeat Step 1 until solved.\n\nBut there is a nuance here; finding the maximum matching in step 1 on each iteration will cause the algorithm to become $O(n^5)$. In order to avoid this, on each step we can just modify the matching from the previous step, which only takes $O(n^2)$ operations.\n\nIt’s easy to see that no more than $n^2$ iterations will occur, because every time at least one edge becomes 0-weight. Therefore, the overall complexity is $O(n^4)$\n\n## $O(n^3)$ Algorithm Explanation","fields":{"timeToRead":{"minutes":3.385},"slug":"/posts/2022-08-01/hungarian-algorithm/"},"internal":{"contentFilePath":"/Users/jeongdongha/code/toy/ha4219.github.io/contents/posts/2022-08-01/hungarian-algorithm.mdx"}},"next":{"fields":{"slug":"/posts/2022-01-20/test/"}},"previous":{"fields":{"slug":"/posts/2022-10-01/positional-embedding/"}}},{"node":{"frontmatter":{"title":"algorithm","date":"2022-01-22T01:55:00.000Z","moment":"a year ago","thumbnail":null,"tags":["boj"],"category":"algorithm"},"excerpt":"testtest","id":"cc9b011d-9836-5741-bb74-70d9aa2a7a9c","body":"\n\n# test\n\n## testtest","fields":{"timeToRead":{"minutes":0.02},"slug":"/posts/2022-01-20/test/"},"internal":{"contentFilePath":"/Users/jeongdongha/code/toy/ha4219.github.io/contents/posts/2022-01-20/test.mdx"}},"next":{"fields":{"slug":"/posts/2022-03-01/test-algorithm/"}},"previous":{"fields":{"slug":"/posts/2022-08-01/hungarian-algorithm/"}}},{"node":{"frontmatter":{"title":"test-algorithm","date":"2022-01-22T01:55:00.000Z","moment":"a year ago","thumbnail":"https://wikidocs.net/images/page/162976/FPN_2.png","tags":["boj"],"category":"algorithm"},"excerpt":"한글 작성 테스트한글 작성 테스트한글 작성 테스트한글 작성 테스트한글 작성 테스트한글 작성 테스트한글 작성 테스트한글 작성 테스트 // main.cpp\n\n#include…","id":"f054da06-05b0-546c-ab11-0bfd200f2926","body":"\n\n# algorithm 어렵다~~~~\n\n## 한글 작성 테스트한글 작성 테스트한글 작성 테스트한글 작성 테스트한글 작성 테스트한글 작성 테스트한글 작성 테스트한글 작성 테스트\n\n\n```cpp\n// main.cpp\n\n#include <iostream>\n\nint main() {\n  std::cout<<\"Hello World!\\n\";\n  return 0;\n}\n```\n\n\n$a=1$","fields":{"timeToRead":{"minutes":0.38},"slug":"/posts/2022-03-01/test-algorithm/"},"internal":{"contentFilePath":"/Users/jeongdongha/code/toy/ha4219.github.io/contents/posts/2022-03-01/test-algorithm.mdx"}},"next":{"fields":{"slug":"/posts/2022-02-10/fpn/"}},"previous":{"fields":{"slug":"/posts/2022-01-20/test/"}}},{"node":{"frontmatter":{"title":"feature pyramid network with yolov3","date":"2022-01-20T01:55:00.000Z","moment":"a year ago","thumbnail":"https://wikidocs.net/images/page/162976/FPN_2.png","tags":["fpn"],"category":"vision"},"excerpt":"Abstract Feature pyramids are a basic component in recognition\nsystems for detecting objects at…","id":"900e56ab-eee7-5794-8f3f-b11e679d8a9f","body":"\n\n\n# Feature Pyramid Networks for Object Detection\n\n\n\n## Abstract\n\n```bash\nFeature pyramids are a basic component in recognition\nsystems for detecting objects at different scales. But recent\ndeep learning object detectors have avoided pyramid representations, in part because they are compute and memory\nintensive. In this paper, we exploit the inherent multi-scale,\npyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for\nbuilding high-level semantic feature maps at all scales. This\narchitecture, called a Feature Pyramid Network (FPN),\nshows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster\nR-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without\nbells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 6 FPS on a GPU\nand thus is a practical and accurate solution to multi-scale\nobject detection. Code will be made publicly available.\n```\n\n- FPN은 여러가지 scale을 가진 객체탐지에 기본적인 component이다. 하지만 최근 다른 연구에서는 복잡하고 메모리 집중적이기 때문에 pyramid representations 피한다. 본 논문에서, 한계 추가 비용으로 feature pyramid를 구성하기 위해 고유의 multi-scale, 즉 pyramidal hierarchy of deep convolutional networks(convolutional network의 피라미드 계층)을 활용한다. All scale에 high-level feature maps에 대한 top-down 구조는 개발됐다.  FPN은  몇가지 application의 generic feature extractor에 중요한 성장을 보여준다. FPN을 사용한 Faster-RCNN은 without bells and whistles, COCO detection benchmark에서 최첨단? 단일 모델 결과를 달성하여 COCO 2016 challenge 우승자를 포함한 모든 기존 단일 모델 항목을 능가한다. 추가로 우리 모델은 GPU 환경에서 6 FPS를 갖는다. 따라서 multi-scale 객체탐지에 실용적이고 정확한 해결책이다. code는 곧 공식적으로 이용 가능하다.\n\n## 1. Introduction\n\n```bash\n Recognizing objects at vastly different scales is a fundamental challenge in computer vision. Feature pyramids built upon image pyramids (for short we call these featurized image pyramids) form the basis of a standard solution [1] (Fig. 1(a)). These pyramids are scale-invariant in the sense that an object’s scale change is offset by shifting its\nlevel in the pyramid. Intuitively, this property enables a model to detect objects across a large range of scales by scanning the model over both positions and pyramid levels.\n Featurized image pyramids were heavily used in the era of hand-engineered features [5, 25]. They were so critical that object detectors like DPM [7] required dense scale sampling to achieve good results (e.g., 10 scales per octave). For recognition tasks, engineered features have largely been replaced with features computed by deep convolutional networks (ConvNets) [19, 20]. Aside from being capable of representing higher-level semantics, ConvNets are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)). But even with this robustness, pyramids are still needed to get the most accurate results. All recent top entries in the ImageNet [33] and COCO [21] detection challenges use multi-scale testing on featurized image pyramids (e.g., [16, 35]). The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.\n Nevertheless, featurizing each level of an image pyramid has obvious limitations. Inference time increases considerably (e.g., by four times [11]), making this approach\nimpractical for real applications. Moreover, training deep \n```\n\n- 매우 다른 sacles에 객체를 인식하는 것은 CV에 기초적인 과제이다. image pyramids에서 형성된 Feature pyramids는 기본적인 해결책의 근간을 형성한다.  pyramids는 물체의 스케일 변화가 다음 값을 이동함으로써 상쇄된다는 점에서 scale-불변이다. 직관적으로, this property를 사용하면 모델이 위치 및 피라미드 레벨 모두에서 모델을 스캔하여 광범위한 축척에 걸쳐 객체를 탐지할 수 있습니다.\n- Featurized image pyramids는 hand-engineered features에서 많이 사용된다. DPM과 같은 object detectors가 좋은 결과를 내기 위해 dense scale sampling이 필요한 것은 치명적이다. 인식 과제에서, engineered된 feature들은 deep ConvNets으로 계산된 features으로 바뀌었다. 더 높은 semantics를 나타낼 뿐만 아니라, ConvNets scale 변화에 강력하다. 따라서 single input scale에서 계산된 features로 인식을 용이하게 한다. 강건하지만 pyramids는 여전히 가장 좋은 정확도 결과를 필요로 한다. 최근 유명 대회에 참가하는 모델들은 featurized image pyramids에 multi-scale testing을 사용한다. 각 level에 image pyramid를 featurizing의  주요 이점은 고해상도를 포함한 모든 level에서 semantically strong한 multi-scale feature 를 제공한다.\n- 그럼에도 불구하고 명백한 한계가 존재한다. 시간이 상당히 증가하고 실제 app에서 비실용적일 것이라고 예측된다. 게다가 end-to-end networks에 올리는 것은 메모리적으로 실현 불가능하고 image pyramid는 오직 test time에 사용되기에 train과 test-time 추론 사이에 모순이 발생한다. 이러한 이유로 Fast, Faster R-CNN은 featurized image pyramids를 기본 세팅에 사용하지 않기로 선택했다. \n- 하지만 image pyrmaid는 오직 multi-scale feature representation을 계산하는 것만 있는게 아니다. deep ConvNet은 각 layer마다 feature hierarchy를 계산하고 feature 계층을 subsampling은 고유한 multi-scale과 pyramidal shape를 갖고 있다. 이러한 feature hierarchy는 각기 다른 해상도의 feature map을 제공하지만 깊이에 따라 야기되는 큰 semantic gaps을 보여준다. 높은 해상도 maps은 인식 능력에 안좋은 낮은 feature를 갖고있다.\n- SSD는 ConvNet의 마치 featurized image pyramid처럼 pyramidal feature hierarchy를 처음 사용한 시도 중 하나이다. 이상적으로 SSD-style pyramid는 다른 여러 이전에 통과된 layer로 부터 multi-scale feature maps을 재사용해서 cost가 적다. 그러나 low-level feature을 사용하지 않으려면 SSD는 이미 계산된 hierarchy를 다시 사용하고 대신 새로운 layer를 추가하면서 network의 상위 계층으로부터 pyramid를 구축한다. 따라서 고해상도 계층맵을 사용할 기회를 잃는다. 고해상도 맵은 작은 객체를 탐지할 때 중요하다.\n- 이 논문의 목표는 모든 scale에 강력한 ConvNet's feature hierarchy를 만들어서 pyramid shape에 영향을 주는 것이다. 목표를 달성하려면 낮은 해상도와 높은 해상도의 semantically 한 강한 features 그리고 top-down pathway와 lateral connections을 통과하고 semantically하고 약한 features들 구조에 달려있다.\n\n\n\n\n\n## [Ref](https://youtu.be/r3U9MJslg5g)\n\n정말 많이 보고 배워서 참고합니다. 또한 영상 내용을 시간을 바탕으로 제가 새로 재구성했습니다.\n\n\n\n- **background**\n  - **Resolution && level feature**\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932669533649535026/unknown.png)\n    초반 layer에서는 높은 해상도를 갖고 있지만 각 pixel하나는 큰 의미가 없을 수 있다. (매우 작은 object면 있을 수도 있음. 하지만 대부분 object들은 거의 pixel 하나에만 들어있지 않고 across하기 때문에 그럴리는 적다.) FC를 제외하고 뒤로가면 갈수록 해상도는 낮아지지만 depth가 깊어지고 각 pixel에 담는 정보는 매우 의미있음을 알 수 있다.\n    여기서 pixel은 x.shape = (512,  w, h)로 봤을때 x[:, p_x, p_y] 값을 의미한다. 또한 FC를 제외하고 생각하자.\n  - **Scale**\n    - Small Scale => 나무\n    - Large Scale => 숲\n      ![img](https://cdn.discordapp.com/attachments/904717831940239403/932661190805561415/unknown.png)\n    - 비교\n      ![img](https://cdn.discordapp.com/attachments/904717831940239403/932662734804357160/unknown.png)\n      - 그러면 객체를 인식하기 위해서는 Large Scale을 만드는 것이 좋을 것이다. 그러기 위한 방법으로는 2가지가 있다.\n      \t1. Window Size를 늘린다. (맨 오른쪽 그림)\n      \t2. Image Size를 줄인다. (3번째 그림)\n      - 그림에서는 window 크기를 늘린 것과 image 크기를 줄이는게 차이가 있어보이지만 사실은 차이가 없다. 의미만 부여하기 위해 만들었다.\n- **Paper list**\n  ![img](https://github.com/hoya012/deep_learning_object_detection/raw/master/assets/deep_learning_object_detection_history.PNG)\n- **Object Detection Milestones**\n  ![img](https://user-images.githubusercontent.com/31475037/75324222-d2612f80-58b9-11ea-8ae5-e1ad0e1ccfd5.png)\n- 이전에 object detection에는 one-stage detector와 two-stage dectector가 있다고 소개했습니다.\n  - **1-Stage Detector**는 Region Proposal -> Classification이 순서대로 실행\n  - **2-Stage Detector**는 Region Proposal, Classification이 동시에 실행\n\n\n\n## FPN\n\n![FPN Explained | Papers With Code](https://production-media.paperswithcode.com/methods/new_teaser_TMZlD2J.jpg)\n 이 그림은 FPN을 검색하면 나오는 대표적인 그림이다. 이를 바탕으로 설명한다.\n- **Featurized image pyramid**\n  ![img](https://cdn.discordapp.com/attachments/904717831940239403/932665573559640134/unknown.png)\n  위 background에서 설명했던 내용처럼 Large Scale을 맞추기 위해서는 방법이 두 가지가 있다. 하나는 Window box를 늘리거나 또 하나는 Image size를 조정하는 것이다. **Featurized image pyramid**는 후자이다. 따라서 Image size를 변경해서 예측한다. 하지만 이는 자원소모가 크고 빠른 속도를 기대할 수 없다. 따라서 다른 방식이 나오기 시작한다. 해당 방법을 쓴 model은 **Overfeat, HOG, SIFT** 등이 있다.\n  ![img](https://blog.kakaocdn.net/dn/mr4Lo/btqPe13oMhP/pK4EWkPsTbHELwJGcdMIT0/img.jpg)\n- **Single feature map**\n  하나의 image를 넣어서 사용하는 방법이다.  속도 측면에서는 좋지만 이전 방법보다는 정확도가 떨어진다는 단점이 있다. 가장 대표적으로 **Yolov1**이 있다.\n  ![img](https://curt-park.github.io/images/yolo/Figure3.JPG)\n- **Pyramidal feature hierarchy**\n  ![img](https://cdn.discordapp.com/attachments/904717831940239403/932670820235509810/unknown.png)\n  \"그러면 Large Scale을 적용하는 방법으로 Image 크기를 줄이는 법이 있는데 VGG같은 경우 일정 비율로 pooling 되니까 이를 stage마다 적용하는 방법이 있지 않을까?\" 에서 시작했다고 생각한다. 가장 대표적인 모델은 **SSD**가 있다.\n  ![SSD-Sface : Single shot multibox detector for small faces | Semantic Scholar](https://d3i71xaburhd42.cloudfront.net/4e27fec1703408d524d6b7ed805cdb6cba6ca132/19-Figure3.1-1.png)\n  근데 아까 **High Resolution이면 low feature**를 갖는다고 했고 **Low Resolution이면 high feature**를 갖는다고 했다.  forward 도중 predict를 진행하기에 cost 측면에서는 좋다. 하지만 높은 해상도에서는 좋은 feature들을 얻을 수 없다. 그렇기에 높은 해상도에서 관측 가능한 작은 객체들을 탐지하기 어려울 것이다.\n  그래서 여기서 하나 단어를 잡고 가면 **Semantic** Gap이다.\n  ![img](https://cdn.discordapp.com/attachments/904717831940239403/932675643722838077/unknown.png)\n- **Feature Pyramid Network**\n  ![img](https://cdn.discordapp.com/attachments/904717831940239403/932676070908502046/unknown.png)\n  그래서 나온게 FPN이다.  그러면 봐야하는 특징이 세 가지 있다.(Yolov3 모델로 설명할 것이다. 사실 완벽한 FPN은 아닌 것 같다.)\n  - bottom-up pathway\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932682821896667236/unknown.png)\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932682880541392977/unknown.png)\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932678088419061810/unknown.png)\n    해당 부분은 downsampling 과정이고 해상도를 낮추면서 feature level을 높이는 단계이다. FNP 논문에서는 ResNet을 기준으로 설명했다.\n  - top-down pathway\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932678088419061810/unknown.png)\n    해당 부분은 upsampling 과정이다. 해상도를 높이는데 feature를 갖고 간다. 방법은 nn.Upsampling 공식 문서를 확인해도 되지만 간략히 설명하면 neareast는 아래와 같은 방식으로 동작한다.\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932680429964787712/unknown.png)\n  - lateral connections\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932683196317970502/unknown.png)\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932678851274879056/unknown.png)\n    여기 부분이 yolov3와 다르다. FNP는 +를 한다. 하지만 yolov3는 depth기준 concat을 수행한다.\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932681564372992052/unknown.png)\n    해당 부분이 어떤 의미가 있는지는 좀 더 찾아보겠다.\n    본 논문에서는 아래와 같은 + 연산으로 1x1 conv에서 output 값을 맞춰준다. 따라서 같은 shape로 + 연산이 이루어진다.\n    ![img](https://blog.kakaocdn.net/dn/0ela2/btqUsdFXuAe/zSFO8k1p1JIbMoz5vWi75k/img.png)\n\n\n\n\n\n## Region Proposal Network(RPN)\n\n해당 내용은 **Faster-RCNN**에서 나온 내용이다. **Yolov2**에서 anchor는 위 논문을 보고 따왔다. \n\n```bash\n3.1 Region Proposal Networks\nA Region Proposal Network (RPN) takes an image\n(of any size) as input and outputs a set of rectangular\nobject proposals, each with an objectness score.3 We\nmodel this process with a fully convolutional network\n[7], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN\nobject detection network [2], we assume that both nets\nshare a common set of convolutional layers. In our experiments, we investigate the Zeiler and Fergus model\n[32] (ZF), which has 5 shareable convolutional layers\nand the Simonyan and Zisserman model [3] (VGG-16),\nwhich has 13 shareable convolutional layers.\nTo generate region proposals, we slide a small\nnetwork over the convolutional feature map output\nby the last shared convolutional layer. This small\nnetwork takes as input an n × n spatial window of\nthe input convolutional feature map. Each sliding\nwindow is mapped to a lower-dimensional feature\n(256-d for ZF and 512-d for VGG, with ReLU [33]\nfollowing). This feature is fed into two sibling fullyconnected layers—a box-regression layer (reg) and a\nbox-classification layer (cls). We use n = 3 in this\npaper, noting that the effective receptive field on the\ninput image is large (171 and 228 pixels for ZF and\nVGG, respectively). This mini-network is illustrated\nat a single position in Figure 3 (left). Note that because the mini-network operates in a sliding-window\nfashion, the fully-connected layers are shared across\nall spatial locations. This architecture is naturally implemented with an n×n convolutional layer followed\nby two sibling 1 × 1 convolutional layers (for reg and\ncls, respectively).\n3.1.1 Anchors\nAt each sliding-window location, we simultaneously\npredict multiple region proposals, where the number\nof maximum possible proposals for each location is\ndenoted as k. So the reg layer has 4k outputs encoding\nthe coordinates of k boxes, and the cls layer outputs\n2k scores that estimate probability of object or not\nobject for each proposal4\n. The k proposals are parameterized relative to k reference boxes, which we call\nanchors. An anchor is centered at the sliding window\nin question, and is associated with a scale and aspect\nratio (Figure 3, left). By default we use 3 scales and\n3 aspect ratios, yielding k = 9 anchors at each sliding\nposition. For a convolutional feature map of a size\nW × H (typically ∼2,400), there are W Hk anchors in\ntotal.\n```\n","fields":{"timeToRead":{"minutes":15.37},"slug":"/posts/2022-02-10/fpn/"},"internal":{"contentFilePath":"/Users/jeongdongha/code/toy/ha4219.github.io/contents/posts/2022-02-10/fpn.mdx"}},"next":null,"previous":{"fields":{"slug":"/posts/2022-03-01/test-algorithm/"}}}]}},"staticQueryHashes":[],"slicesMap":{}}