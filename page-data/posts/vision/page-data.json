{"componentChunkName":"component---src-templates-category-tsx","path":"/posts/vision/","result":{"pageContext":{"currentCategory":"vision","categories":["All","daily","review","devops","vision","algorithm"],"edges":[{"node":{"frontmatter":{"title":"Positional Encoding(Embedding)ì´ í•„ìš”í•œ ì´ìœ ","date":"2022-10-01T00:00:00.000Z","moment":"4 months ago","thumbnail":null,"tags":["vision","nlp"],"category":"vision"},"excerpt":"Since our model contains no recurrence and no convolution, in order for the model to make use ofâ€¦","id":"30f06b86-3005-5315-bc2e-a6a4893b018d","body":"\n> Since our model contains **no recurrence and no convolution**, in order for **the model to make use of the order of the sequence**, we must inject some information about the relative or absolute position of the tokens in the sequence.\n> \n\n> RNNê³¼ LSTMê³¼ ë‹¤ë¥´ê²Œ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” **ì…ë ¥ ìˆœì„œê°€ ë‹¨ì–´ ìˆœì„œì— ëŒ€í•œ ì •ë³´ë¥¼ ë³´ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤.** ë‹¤ì‹œ ë§í•˜ë©´, íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ê²½ìš° ì‹œí€€ìŠ¤ê°€ í•œë²ˆì— ë³‘ë ¬ë¡œ ì…ë ¥ë˜ê¸°ì— ë‹¨ì–´ ìˆœì„œì— ëŒ€í•œ ì •ë³´ê°€ ì‚¬ë¼ì§„ë‹¤. ë”°ë¼ì„œ ë‹¨ì–´ ìœ„ì¹˜ ì •ë³´ë¥¼ ë³„ë„ë¡œ ë„£ì–´ì¤˜ì•¼ í•œë‹¤.\n> \n\n- Positional Encoding, Embedding ë‘˜ ë‹¤ ì…ë ¥ ìœ„ì¹˜ì— ëŒ€í•œ ê°’ì„ ì¶”ê°€ì‹œí‚¤ê¸° ìœ„í•œ ëª©ì ì´ê¸°ì— ì•„ë˜ì—ì„œëŠ” PEë¡œ í†µì¼\n\n## ë¬¸ì œ ì œê¸°\n\n![Untitled](pe/Untitled.png)\n\n ìœ„ì™€ ê°™ì´ **transformer** ì…ë ¥ì´ ì£¼ì–´ì§ˆ ê²ƒì´ë‹¤. ì´ë•Œ widthëŠ” embedding ì°¨ì›, heightì€ vocab_sizeê°€ ë  ê²ƒì´ë‹¤. ì²˜ìŒ ìƒê°ì€ ìœ„ì™€ ê°™ì€ ë°ì´í„°ê°€ ë“¤ì–´ì˜¨ë‹¤ë©´ ê° í–‰ì€ ë‹¨ì–´ì— ê´€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆê³  **í–‰ì˜ ìˆœì„œëŠ” ê³§ ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì— PEê°€ êµ³ì´ í•„ìš”í•œê°€**ë¼ëŠ” ìƒê°ì´ ìˆì—ˆë‹¤.\n\n## ë¬¸ì œ í•´ê²°\n\n### ë°ì´í„°ì…‹\n\nì´ì— ëŒ€í•´ ê°„ë‹¨í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„¤ëª…í•˜ê² ë‹¤.\n\n```python\nMAX = 1000\n\ndata = []\n\nfor i in range(MAX):\n    for j in range(MAX):\n        data.apend(f'{i}+{j}' + f'={i+j}' + '\\n')\n        data.apend(f'{i}-{j}' + f'={i-j}' + '\\n')\n\nwith open('dataset/number_data.txt', 'w') as f:\n    f.writelines(data)\n```\n\nìœ„ ì½”ë“œë¡œ 2,000,000 ê°œì˜ ë°ì´í„°ë¥¼ ìƒì„±í–ˆë‹¤. ë§ì…ˆê³¼ ëº„ì…ˆì‹ì„ ë„£ì—ˆì„ ë•Œ ê²°ê³¼ë¥¼ **transformer** ëª¨ë¸ì´ í•™ìŠµí•´ ì´ë¥¼ ìœ ì¶”í•˜ëŠ” ëª¨ë¸ì´ë‹¤. (í•™ìŠµ ëª¨ë¸ ë° ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì • ì½”ë“œ ìƒëµ)\n\n### Attention\n\n$$Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt d_k}\\right)V$$\n\nattentionì´ ì´ë¥¼ ì˜ ê´€ì°°í•˜ëŠ”ê°€ê°€ ë¬¸ì œì´ë‹¤. ë”°ë¼ì„œ ì…ë ¥ â€œ21+211=232â€ì— ëŒ€í•œ $softmax\\left(\\frac{QK^T}{\\sqrt d_k}\\right)$ ê°’(ê°€ì¤‘ì¹˜)ì„ ì‹œê°í™”í•´ ë³´ê² ë‹¤.\n\n- y axis: Query, x axis: Key\n\n1. PE ì ìš©\n    1. Multi-Head Attention in Decoder(Query: Decoder output, Key: Encoder output)\n        ![Untitled](pe/Untitled-1.png)\n        ë¹¨ê°„ Boxë¥¼ í™•ì¸í•´ë³´ê² ë‹¤.\n        ìµœì¢…ì ìœ¼ë¡œ ì´ëŠ” ì„¤ëª…í•˜ê¸° ì–´ë µë‹¤. ì•„ë˜ ë‹¤ë¥¸ ì…ë ¥ì— ëŒ€í•´ì„œëŠ” ì„¤ëª…í•˜ê¸° ì‰½ê²Œ ê°€ì¤‘ì¹˜ê°€ ì¶œë ¥ë˜ì§€ë§Œ ìœ„ ì…ë ¥ì— ëŒ€í•´ì„œëŠ” ì´í•´í•˜ê¸° í˜ë“  ê²°ê³¼ê°€ ë‚˜ì™”ë‹¤. í•˜ì§€ë§Œ ë’¤ì— ë‚˜ì˜¤ëŠ” ê²°ê³¼ê°€ ì˜ë¯¸ê°€ ìˆê¸°ì— ì´ë¥¼ ë¬µì‹œí•˜ê³  ë„˜ì–´ê°„ë‹¤.\n        ![Untitled](pe/Untitled-2.png)\n    2. Multi-Head Attention in Encoder(Query, Key: Encoder output)\n        ![Untitled](pe/Untitled-3.png)\n        ![Untitled](pe/Untitled-4.png)\n        ë‘ ë²ˆì§¸ ê·¸ë¦¼ì€ ì²« ë²ˆì§¸ ê·¸ë˜í”„ì—ì„œ ë¹¨ê°„ Boxë¥¼ í™•ëŒ€ì‹œí‚¨ ê²ƒì´ë‹¤.\n        ì´ ë˜í•œ ëª¨ë“  í–‰ì˜ ëŒ€í•´ ì„¤ëª…í•˜ê¸°ëŠ” ì–´ë µì§€ë§Œ ëŒ€ì²´ë¡œ ì´í•´í•˜ê¸° ì‰¬ìš´ ê²°ê³¼ë¥¼ ë‚´ê³  ìˆë‹¤. í•˜ì§€ë§Œ ëª¨ë“  Multi-Headì—ì„œ ê´€ì¸¡ëœ ê°€ì¤‘ì¹˜ ê°’ì—ì„œ ê³µí†µì ì¸ ë‚´ìš©ì´ ìˆëŠ”ë° ì´ëŠ” **ì‹­ì˜ ìë¦¬ â€œ1â€ê³¼ ì¼ì˜ ìë¦¬ â€œ1â€ì„ ê·¸ë¦¬ê³  ì‹­ì˜ ìë¦¬ â€œ2â€ì™€ ë°±ì˜ ìë¦¬ â€œ2â€ë¥¼ êµ¬ë¶„í•œë‹¤**ëŠ” ê²ƒì´ë‹¤. ë§Œì•½ PEê°€ ì—†ìœ¼ë©´ ê°€ëŠ¥í• ê¹Œ ì•„ë˜ í‘œë¥¼ ë” í™•ì¸í•´ë³´ì.\n2. PE ë¯¸ì ìš©\n    1. Multi-Head Attention in Decoder(Query: Decoder output, Key: Encoder output)\n        ![Untitled](pe/Untitled-5.png)\n        PEë¥¼ ì ìš©í–ˆì„ ë•Œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì´ ë˜í•œ í•´ì„í•˜ê¸° ì–´ë ¤ìš´ ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤.\n    2. Multi-Head Attention in Encoder(Query, Key: Encoder output)\n        ![Untitled](pe/Untitled-6.png)\n        ![Untitled](pe/Untitled-7.png)\n        ìœ„ ë˜í•œ ì„¤ëª…í•˜ê¸° ì–´ë ¤ìš´ ê²°ê³¼ë¥¼ ê°–ê³  ìˆì§€ë§Œ 8ê°œì˜ Multi-Headì—ì„œ ë³´ì´ëŠ” ê³µí†µì ì¸ íŠ¹ì§•ì€ **ì‹­ì˜ ìë¦¬ â€œ1â€ê³¼ ì¼ì˜ ìë¦¬ â€œ1â€ì„ ê·¸ë¦¬ê³  ì‹­ì˜ ìë¦¬ â€œ2â€ì™€ ë°±ì˜ ìë¦¬ â€œ2â€ë¥¼ êµ¬ë¶„í•˜ì§€ ëª»í•˜ê³  ê°–ì€ ê°€ì¤‘ì¹˜ë¥¼ ê°–ëŠ”ë‹¤.** ê·¸ëŸ¬ë©´ ì™œ êµ¬ë¶„í•˜ì§€ ëª»í• ê¹Œ?\n\n<Callout>\nğŸ’¡ ìœ„ ë‚´ìš©ì„ ìš”ì•½í•˜ë©´ **PE**ë¥¼ ì œê±°í•œ Encoder ë¶€ë¶„ì˜ Self Attentionì—ì„œ ê°™ì€ ë¬¸ìì—´ì´ ë“¤ì–´ì˜¤ë©´ **ìë¦¬ìˆ˜ë¥¼ êµ¬ë¶„í•˜ì§€ ëª»í•˜ê³  ê°™ì€ ê°€ì¤‘ì¹˜ë¥¼ ë½‘ì•„ë‚¸ë‹¤**ëŠ” ë¬¸ì œê°€ ìˆë‹¤.\n</Callout>\n\n### í–‰ë ¬ ê³„ì‚°\n\nì´ëŠ” $softmax\\left(\\frac{QK^T}{\\sqrt d_k}\\right)$ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì—ì„œ ì´í•´í•  ìˆ˜ ìˆë‹¤.\n\n![Untitled](pe/Untitled-8.png)\n\n ë¨¼ì € **Query, Key, Value**ëŠ” ì…ë ¥ $x$ì— ëŒ€í•´ ê°€ì¤‘ì¹˜ **W**ë¥¼ ì´ìš©í•´ ìƒì„±ëœë‹¤. ì²˜ìŒ Encoderì—ì„œ ìƒê°í•´ë³´ë©´ $x$ì˜ ê° í–‰ì€ ë‹¨ì–´ë¥¼ ì˜ë¯¸í•  ê²ƒì´ê³  í–‰ë ¬ê³± ì—°ì‚°ì„ ì ìš©í•œ í›„ ë‚˜ì˜¨ matrixì˜ ê° í–‰ë„ $x$ì˜ í–‰ ê°’ì— ì˜í–¥ì´ ë§ì„ ê²ƒì´ë‹¤. ì…ë ¥ì´ **â€œ12+1â€**ì´ë¼ë©´ í–‰ë ¬ê³±ìœ¼ë¡œ ìƒì„±ëœ $Query, Key$ë¥¼ ìœ„ì™€ ê°™ë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.\n\n $X$ì˜ ì‹­ì˜ ìë¦¬ â€œ1â€ê³¼ ì¼ì˜ ìë¦¬ â€œ1â€ì€ ê°™ì€ ê°’ì„ ê°–ì„ ê²ƒì´ë‹¤.(Word Embeddingì˜ ê²°ê³¼) ë”°ë¼ì„œ ê°ê°ì˜ $Query, Key$ í–‰ë ¬ì—ì„œ ì‹­ì˜ ìë¦¬ â€œ1â€ê³¼ ì¼ì˜ ìë¦¬ â€œ1â€ì€ ê°™ì€ ê°’ì„ ê°–ì„ ê²ƒì´ë‹¤.\n\n![Untitled](pe/Untitled-9.png)\n\n![Untitled](pe/Untitled-10.png)\n\nì´í›„ $energy$ê°’ì„ êµ¬í•´ë„ ì‹­ì˜ ìë¦¬ â€œ1â€ê³¼ ì¼ì˜ ìë¦¬ â€œ1â€ ë‘ í–‰ì˜ ê°’ì€ ë™ì¼í•˜ë‹¤ë¼ëŠ” ê²ƒì„ ìƒê°í•  ìˆ˜ ìˆê³  ì´í›„ ì—°ì‚°ë„ ë™ì¼í•˜ë‹¤.\n\nê²°ë¡ ì ìœ¼ë¡œ ë‚˜ì˜¨ ê°€ì¤‘ì¹˜ ê°’ì€ Attention mechanismì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ $Value$ì— ëŒ€í•´ ì–´ë–¤ ë‹¨ì–´ë¥¼ ì§‘ì¤‘í• ì§€ë¥¼ ë³´ì—¬ì¤„ í…ë° ê°€ì¤‘ì¹˜ $\\alpha$ì—ì„œ ì´ë¯¸ ì´ ë‘ ê°œì˜ ì •ë³´ë¥¼ êµ¬ë¶„í•  ìˆ˜ ì—†ê¸°ì— ì´í›„ ë‚˜ì˜¤ëŠ” outputì€ ì´ì— ëŒ€í•´ ëª¨í˜¸í•œ ê²°ê³¼ë¡œ ë‚˜ì˜¬ ê²ƒì´ë‹¤. **ë”°ë¼ì„œ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ë‹¤ë©´ í•™ìŠµí•˜ëŠ” ë° ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆë‹¤.**\n\nìœ„ ë°ì´í„°ì…‹ì€ ë§ì…ˆê³¼ ëº„ì…ˆì— ê´€í•œ ë°ì´í„°ì…‹ì´ì§€ë§Œ ì´ë¥¼ í™•ì¥ì‹œì¼œ ë¬¸ì¥ ë²ˆì—­ taskì—ì„œë„ í•œ ë¬¸ì¥ì•ˆì— ë˜‘ê°™ì€ ë‹¨ì–´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì´ë¥¼ êµ¬ë¶„í•  ìˆ˜ ì—†ì„ ê²ƒì´ë‹¤.\n","fields":{"timeToRead":{"minutes":6.64},"slug":"/posts/2022-10-01/positional-embedding/"},"internal":{"contentFilePath":"/Users/jeongdongha/code/toy/ha4219.github.io/contents/posts/2022-10-01/positional-embedding.mdx"}},"next":{"fields":{"slug":"/posts/2022-08-01/hungarian-algorithm/"}},"previous":{"fields":{"slug":"/posts/2022-12-20/cicd-lecture-memo/"}}},{"node":{"frontmatter":{"title":"feature pyramid network with yolov3","date":"2022-01-20T01:55:00.000Z","moment":"a year ago","thumbnail":"https://wikidocs.net/images/page/162976/FPN_2.png","tags":["fpn"],"category":"vision"},"excerpt":"Abstract Feature pyramids are a basic component in recognition\nsystems for detecting objects atâ€¦","id":"900e56ab-eee7-5794-8f3f-b11e679d8a9f","body":"\n\n\n# Feature Pyramid Networks for Object Detection\n\n\n\n## Abstract\n\n```bash\nFeature pyramids are a basic component in recognition\nsystems for detecting objects at different scales. But recent\ndeep learning object detectors have avoided pyramid representations, in part because they are compute and memory\nintensive. In this paper, we exploit the inherent multi-scale,\npyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for\nbuilding high-level semantic feature maps at all scales. This\narchitecture, called a Feature Pyramid Network (FPN),\nshows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster\nR-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without\nbells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 6 FPS on a GPU\nand thus is a practical and accurate solution to multi-scale\nobject detection. Code will be made publicly available.\n```\n\n- FPNì€ ì—¬ëŸ¬ê°€ì§€ scaleì„ ê°€ì§„ ê°ì²´íƒì§€ì— ê¸°ë³¸ì ì¸ componentì´ë‹¤. í•˜ì§€ë§Œ ìµœê·¼ ë‹¤ë¥¸ ì—°êµ¬ì—ì„œëŠ” ë³µì¡í•˜ê³  ë©”ëª¨ë¦¬ ì§‘ì¤‘ì ì´ê¸° ë•Œë¬¸ì— pyramid representations í”¼í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œ, í•œê³„ ì¶”ê°€ ë¹„ìš©ìœ¼ë¡œ feature pyramidë¥¼ êµ¬ì„±í•˜ê¸° ìœ„í•´ ê³ ìœ ì˜ multi-scale, ì¦‰ pyramidal hierarchy of deep convolutional networks(convolutional networkì˜ í”¼ë¼ë¯¸ë“œ ê³„ì¸µ)ì„ í™œìš©í•œë‹¤. All scaleì— high-level feature mapsì— ëŒ€í•œ top-down êµ¬ì¡°ëŠ” ê°œë°œëë‹¤.  FPNì€  ëª‡ê°€ì§€ applicationì˜ generic feature extractorì— ì¤‘ìš”í•œ ì„±ì¥ì„ ë³´ì—¬ì¤€ë‹¤. FPNì„ ì‚¬ìš©í•œ Faster-RCNNì€ without bells and whistles, COCO detection benchmarkì—ì„œ ìµœì²¨ë‹¨? ë‹¨ì¼ ëª¨ë¸ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ì—¬ COCO 2016 challenge ìš°ìŠ¹ìë¥¼ í¬í•¨í•œ ëª¨ë“  ê¸°ì¡´ ë‹¨ì¼ ëª¨ë¸ í•­ëª©ì„ ëŠ¥ê°€í•œë‹¤. ì¶”ê°€ë¡œ ìš°ë¦¬ ëª¨ë¸ì€ GPU í™˜ê²½ì—ì„œ 6 FPSë¥¼ ê°–ëŠ”ë‹¤. ë”°ë¼ì„œ multi-scale ê°ì²´íƒì§€ì— ì‹¤ìš©ì ì´ê³  ì •í™•í•œ í•´ê²°ì±…ì´ë‹¤. codeëŠ” ê³§ ê³µì‹ì ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•˜ë‹¤.\n\n## 1. Introduction\n\n```bash\n Recognizing objects at vastly different scales is a fundamental challenge in computer vision. Feature pyramids built upon image pyramids (for short we call these featurized image pyramids) form the basis of a standard solution [1] (Fig. 1(a)). These pyramids are scale-invariant in the sense that an objectâ€™s scale change is offset by shifting its\nlevel in the pyramid. Intuitively, this property enables a model to detect objects across a large range of scales by scanning the model over both positions and pyramid levels.\n Featurized image pyramids were heavily used in the era of hand-engineered features [5, 25]. They were so critical that object detectors like DPM [7] required dense scale sampling to achieve good results (e.g., 10 scales per octave). For recognition tasks, engineered features have largely been replaced with features computed by deep convolutional networks (ConvNets) [19, 20]. Aside from being capable of representing higher-level semantics, ConvNets are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)). But even with this robustness, pyramids are still needed to get the most accurate results. All recent top entries in the ImageNet [33] and COCO [21] detection challenges use multi-scale testing on featurized image pyramids (e.g., [16, 35]). The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.\n Nevertheless, featurizing each level of an image pyramid has obvious limitations. Inference time increases considerably (e.g., by four times [11]), making this approach\nimpractical for real applications. Moreover, training deep \n```\n\n- ë§¤ìš° ë‹¤ë¥¸ saclesì— ê°ì²´ë¥¼ ì¸ì‹í•˜ëŠ” ê²ƒì€ CVì— ê¸°ì´ˆì ì¸ ê³¼ì œì´ë‹¤. image pyramidsì—ì„œ í˜•ì„±ëœ Feature pyramidsëŠ” ê¸°ë³¸ì ì¸ í•´ê²°ì±…ì˜ ê·¼ê°„ì„ í˜•ì„±í•œë‹¤.  pyramidsëŠ” ë¬¼ì²´ì˜ ìŠ¤ì¼€ì¼ ë³€í™”ê°€ ë‹¤ìŒ ê°’ì„ ì´ë™í•¨ìœ¼ë¡œì¨ ìƒì‡„ëœë‹¤ëŠ” ì ì—ì„œ scale-ë¶ˆë³€ì´ë‹¤. ì§ê´€ì ìœ¼ë¡œ, this propertyë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì´ ìœ„ì¹˜ ë° í”¼ë¼ë¯¸ë“œ ë ˆë²¨ ëª¨ë‘ì—ì„œ ëª¨ë¸ì„ ìŠ¤ìº”í•˜ì—¬ ê´‘ë²”ìœ„í•œ ì¶•ì²™ì— ê±¸ì³ ê°ì²´ë¥¼ íƒì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- Featurized image pyramidsëŠ” hand-engineered featuresì—ì„œ ë§ì´ ì‚¬ìš©ëœë‹¤. DPMê³¼ ê°™ì€ object detectorsê°€ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚´ê¸° ìœ„í•´ dense scale samplingì´ í•„ìš”í•œ ê²ƒì€ ì¹˜ëª…ì ì´ë‹¤. ì¸ì‹ ê³¼ì œì—ì„œ, engineeredëœ featureë“¤ì€ deep ConvNetsìœ¼ë¡œ ê³„ì‚°ëœ featuresìœ¼ë¡œ ë°”ë€Œì—ˆë‹¤. ë” ë†’ì€ semanticsë¥¼ ë‚˜íƒ€ë‚¼ ë¿ë§Œ ì•„ë‹ˆë¼, ConvNets scale ë³€í™”ì— ê°•ë ¥í•˜ë‹¤. ë”°ë¼ì„œ single input scaleì—ì„œ ê³„ì‚°ëœ featuresë¡œ ì¸ì‹ì„ ìš©ì´í•˜ê²Œ í•œë‹¤. ê°•ê±´í•˜ì§€ë§Œ pyramidsëŠ” ì—¬ì „íˆ ê°€ì¥ ì¢‹ì€ ì •í™•ë„ ê²°ê³¼ë¥¼ í•„ìš”ë¡œ í•œë‹¤. ìµœê·¼ ìœ ëª… ëŒ€íšŒì— ì°¸ê°€í•˜ëŠ” ëª¨ë¸ë“¤ì€ featurized image pyramidsì— multi-scale testingì„ ì‚¬ìš©í•œë‹¤. ê° levelì— image pyramidë¥¼ featurizingì˜  ì£¼ìš” ì´ì ì€ ê³ í•´ìƒë„ë¥¼ í¬í•¨í•œ ëª¨ë“  levelì—ì„œ semantically strongí•œ multi-scale feature ë¥¼ ì œê³µí•œë‹¤.\n- ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ëª…ë°±í•œ í•œê³„ê°€ ì¡´ì¬í•œë‹¤. ì‹œê°„ì´ ìƒë‹¹íˆ ì¦ê°€í•˜ê³  ì‹¤ì œ appì—ì„œ ë¹„ì‹¤ìš©ì ì¼ ê²ƒì´ë¼ê³  ì˜ˆì¸¡ëœë‹¤. ê²Œë‹¤ê°€ end-to-end networksì— ì˜¬ë¦¬ëŠ” ê²ƒì€ ë©”ëª¨ë¦¬ì ìœ¼ë¡œ ì‹¤í˜„ ë¶ˆê°€ëŠ¥í•˜ê³  image pyramidëŠ” ì˜¤ì§ test timeì— ì‚¬ìš©ë˜ê¸°ì— trainê³¼ test-time ì¶”ë¡  ì‚¬ì´ì— ëª¨ìˆœì´ ë°œìƒí•œë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ Fast, Faster R-CNNì€ featurized image pyramidsë¥¼ ê¸°ë³¸ ì„¸íŒ…ì— ì‚¬ìš©í•˜ì§€ ì•Šê¸°ë¡œ ì„ íƒí–ˆë‹¤. \n- í•˜ì§€ë§Œ image pyrmaidëŠ” ì˜¤ì§ multi-scale feature representationì„ ê³„ì‚°í•˜ëŠ” ê²ƒë§Œ ìˆëŠ”ê²Œ ì•„ë‹ˆë‹¤. deep ConvNetì€ ê° layerë§ˆë‹¤ feature hierarchyë¥¼ ê³„ì‚°í•˜ê³  feature ê³„ì¸µì„ subsamplingì€ ê³ ìœ í•œ multi-scaleê³¼ pyramidal shapeë¥¼ ê°–ê³  ìˆë‹¤. ì´ëŸ¬í•œ feature hierarchyëŠ” ê°ê¸° ë‹¤ë¥¸ í•´ìƒë„ì˜ feature mapì„ ì œê³µí•˜ì§€ë§Œ ê¹Šì´ì— ë”°ë¼ ì•¼ê¸°ë˜ëŠ” í° semantic gapsì„ ë³´ì—¬ì¤€ë‹¤. ë†’ì€ í•´ìƒë„ mapsì€ ì¸ì‹ ëŠ¥ë ¥ì— ì•ˆì¢‹ì€ ë‚®ì€ featureë¥¼ ê°–ê³ ìˆë‹¤.\n- SSDëŠ” ConvNetì˜ ë§ˆì¹˜ featurized image pyramidì²˜ëŸ¼ pyramidal feature hierarchyë¥¼ ì²˜ìŒ ì‚¬ìš©í•œ ì‹œë„ ì¤‘ í•˜ë‚˜ì´ë‹¤. ì´ìƒì ìœ¼ë¡œ SSD-style pyramidëŠ” ë‹¤ë¥¸ ì—¬ëŸ¬ ì´ì „ì— í†µê³¼ëœ layerë¡œ ë¶€í„° multi-scale feature mapsì„ ì¬ì‚¬ìš©í•´ì„œ costê°€ ì ë‹¤. ê·¸ëŸ¬ë‚˜ low-level featureì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë ¤ë©´ SSDëŠ” ì´ë¯¸ ê³„ì‚°ëœ hierarchyë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•˜ê³  ëŒ€ì‹  ìƒˆë¡œìš´ layerë¥¼ ì¶”ê°€í•˜ë©´ì„œ networkì˜ ìƒìœ„ ê³„ì¸µìœ¼ë¡œë¶€í„° pyramidë¥¼ êµ¬ì¶•í•œë‹¤. ë”°ë¼ì„œ ê³ í•´ìƒë„ ê³„ì¸µë§µì„ ì‚¬ìš©í•  ê¸°íšŒë¥¼ ìƒëŠ”ë‹¤. ê³ í•´ìƒë„ ë§µì€ ì‘ì€ ê°ì²´ë¥¼ íƒì§€í•  ë•Œ ì¤‘ìš”í•˜ë‹¤.\n- ì´ ë…¼ë¬¸ì˜ ëª©í‘œëŠ” ëª¨ë“  scaleì— ê°•ë ¥í•œ ConvNet's feature hierarchyë¥¼ ë§Œë“¤ì–´ì„œ pyramid shapeì— ì˜í–¥ì„ ì£¼ëŠ” ê²ƒì´ë‹¤. ëª©í‘œë¥¼ ë‹¬ì„±í•˜ë ¤ë©´ ë‚®ì€ í•´ìƒë„ì™€ ë†’ì€ í•´ìƒë„ì˜ semantically í•œ ê°•í•œ features ê·¸ë¦¬ê³  top-down pathwayì™€ lateral connectionsì„ í†µê³¼í•˜ê³  semanticallyí•˜ê³  ì•½í•œ featuresë“¤ êµ¬ì¡°ì— ë‹¬ë ¤ìˆë‹¤.\n\n\n\n\n\n## [Ref](https://youtu.be/r3U9MJslg5g)\n\nì •ë§ ë§ì´ ë³´ê³  ë°°ì›Œì„œ ì°¸ê³ í•©ë‹ˆë‹¤. ë˜í•œ ì˜ìƒ ë‚´ìš©ì„ ì‹œê°„ì„ ë°”íƒ•ìœ¼ë¡œ ì œê°€ ìƒˆë¡œ ì¬êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.\n\n\n\n- **background**\n  - **Resolution && level feature**\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932669533649535026/unknown.png)\n    ì´ˆë°˜ layerì—ì„œëŠ” ë†’ì€ í•´ìƒë„ë¥¼ ê°–ê³  ìˆì§€ë§Œ ê° pixelí•˜ë‚˜ëŠ” í° ì˜ë¯¸ê°€ ì—†ì„ ìˆ˜ ìˆë‹¤. (ë§¤ìš° ì‘ì€ objectë©´ ìˆì„ ìˆ˜ë„ ìˆìŒ. í•˜ì§€ë§Œ ëŒ€ë¶€ë¶„ objectë“¤ì€ ê±°ì˜ pixel í•˜ë‚˜ì—ë§Œ ë“¤ì–´ìˆì§€ ì•Šê³  acrossí•˜ê¸° ë•Œë¬¸ì— ê·¸ëŸ´ë¦¬ëŠ” ì ë‹¤.) FCë¥¼ ì œì™¸í•˜ê³  ë’¤ë¡œê°€ë©´ ê°ˆìˆ˜ë¡ í•´ìƒë„ëŠ” ë‚®ì•„ì§€ì§€ë§Œ depthê°€ ê¹Šì–´ì§€ê³  ê° pixelì— ë‹´ëŠ” ì •ë³´ëŠ” ë§¤ìš° ì˜ë¯¸ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n    ì—¬ê¸°ì„œ pixelì€ x.shape = (512,  w, h)ë¡œ ë´¤ì„ë•Œ x[:, p_x, p_y] ê°’ì„ ì˜ë¯¸í•œë‹¤. ë˜í•œ FCë¥¼ ì œì™¸í•˜ê³  ìƒê°í•˜ì.\n  - **Scale**\n    - Small Scale => ë‚˜ë¬´\n    - Large Scale => ìˆ²\n      ![img](https://cdn.discordapp.com/attachments/904717831940239403/932661190805561415/unknown.png)\n    - ë¹„êµ\n      ![img](https://cdn.discordapp.com/attachments/904717831940239403/932662734804357160/unknown.png)\n      - ê·¸ëŸ¬ë©´ ê°ì²´ë¥¼ ì¸ì‹í•˜ê¸° ìœ„í•´ì„œëŠ” Large Scaleì„ ë§Œë“œëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒì´ë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œëŠ” 2ê°€ì§€ê°€ ìˆë‹¤.\n      \t1. Window Sizeë¥¼ ëŠ˜ë¦°ë‹¤. (ë§¨ ì˜¤ë¥¸ìª½ ê·¸ë¦¼)\n      \t2. Image Sizeë¥¼ ì¤„ì¸ë‹¤. (3ë²ˆì§¸ ê·¸ë¦¼)\n      - ê·¸ë¦¼ì—ì„œëŠ” window í¬ê¸°ë¥¼ ëŠ˜ë¦° ê²ƒê³¼ image í¬ê¸°ë¥¼ ì¤„ì´ëŠ”ê²Œ ì°¨ì´ê°€ ìˆì–´ë³´ì´ì§€ë§Œ ì‚¬ì‹¤ì€ ì°¨ì´ê°€ ì—†ë‹¤. ì˜ë¯¸ë§Œ ë¶€ì—¬í•˜ê¸° ìœ„í•´ ë§Œë“¤ì—ˆë‹¤.\n- **Paper list**\n  ![img](https://github.com/hoya012/deep_learning_object_detection/raw/master/assets/deep_learning_object_detection_history.PNG)\n- **Object Detection Milestones**\n  ![img](https://user-images.githubusercontent.com/31475037/75324222-d2612f80-58b9-11ea-8ae5-e1ad0e1ccfd5.png)\n- ì´ì „ì— object detectionì—ëŠ” one-stage detectorì™€ two-stage dectectorê°€ ìˆë‹¤ê³  ì†Œê°œí–ˆìŠµë‹ˆë‹¤.\n  - **1-Stage Detector**ëŠ” Region Proposal -> Classificationì´ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n  - **2-Stage Detector**ëŠ” Region Proposal, Classificationì´ ë™ì‹œì— ì‹¤í–‰\n\n\n\n## FPN\n\n![FPN Explained | Papers With Code](https://production-media.paperswithcode.com/methods/new_teaser_TMZlD2J.jpg)\n ì´ ê·¸ë¦¼ì€ FPNì„ ê²€ìƒ‰í•˜ë©´ ë‚˜ì˜¤ëŠ” ëŒ€í‘œì ì¸ ê·¸ë¦¼ì´ë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…í•œë‹¤.\n- **Featurized image pyramid**\n  ![img](https://cdn.discordapp.com/attachments/904717831940239403/932665573559640134/unknown.png)\n  ìœ„ backgroundì—ì„œ ì„¤ëª…í–ˆë˜ ë‚´ìš©ì²˜ëŸ¼ Large Scaleì„ ë§ì¶”ê¸° ìœ„í•´ì„œëŠ” ë°©ë²•ì´ ë‘ ê°€ì§€ê°€ ìˆë‹¤. í•˜ë‚˜ëŠ” Window boxë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ë˜ í•˜ë‚˜ëŠ” Image sizeë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒì´ë‹¤. **Featurized image pyramid**ëŠ” í›„ìì´ë‹¤. ë”°ë¼ì„œ Image sizeë¥¼ ë³€ê²½í•´ì„œ ì˜ˆì¸¡í•œë‹¤. í•˜ì§€ë§Œ ì´ëŠ” ìì›ì†Œëª¨ê°€ í¬ê³  ë¹ ë¥¸ ì†ë„ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ ë‹¤ë¥¸ ë°©ì‹ì´ ë‚˜ì˜¤ê¸° ì‹œì‘í•œë‹¤. í•´ë‹¹ ë°©ë²•ì„ ì“´ modelì€ **Overfeat, HOG, SIFT** ë“±ì´ ìˆë‹¤.\n  ![img](https://blog.kakaocdn.net/dn/mr4Lo/btqPe13oMhP/pK4EWkPsTbHELwJGcdMIT0/img.jpg)\n- **Single feature map**\n  í•˜ë‚˜ì˜ imageë¥¼ ë„£ì–´ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤.  ì†ë„ ì¸¡ë©´ì—ì„œëŠ” ì¢‹ì§€ë§Œ ì´ì „ ë°©ë²•ë³´ë‹¤ëŠ” ì •í™•ë„ê°€ ë–¨ì–´ì§„ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ê°€ì¥ ëŒ€í‘œì ìœ¼ë¡œ **Yolov1**ì´ ìˆë‹¤.\n  ![img](https://curt-park.github.io/images/yolo/Figure3.JPG)\n- **Pyramidal feature hierarchy**\n  ![img](https://cdn.discordapp.com/attachments/904717831940239403/932670820235509810/unknown.png)\n  \"ê·¸ëŸ¬ë©´ Large Scaleì„ ì ìš©í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ Image í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ë²•ì´ ìˆëŠ”ë° VGGê°™ì€ ê²½ìš° ì¼ì • ë¹„ìœ¨ë¡œ pooling ë˜ë‹ˆê¹Œ ì´ë¥¼ stageë§ˆë‹¤ ì ìš©í•˜ëŠ” ë°©ë²•ì´ ìˆì§€ ì•Šì„ê¹Œ?\" ì—ì„œ ì‹œì‘í–ˆë‹¤ê³  ìƒê°í•œë‹¤. ê°€ì¥ ëŒ€í‘œì ì¸ ëª¨ë¸ì€ **SSD**ê°€ ìˆë‹¤.\n  ![SSD-Sface : Single shot multibox detector for small faces | Semantic Scholar](https://d3i71xaburhd42.cloudfront.net/4e27fec1703408d524d6b7ed805cdb6cba6ca132/19-Figure3.1-1.png)\n  ê·¼ë° ì•„ê¹Œ **High Resolutionì´ë©´ low feature**ë¥¼ ê°–ëŠ”ë‹¤ê³  í–ˆê³  **Low Resolutionì´ë©´ high feature**ë¥¼ ê°–ëŠ”ë‹¤ê³  í–ˆë‹¤.  forward ë„ì¤‘ predictë¥¼ ì§„í–‰í•˜ê¸°ì— cost ì¸¡ë©´ì—ì„œëŠ” ì¢‹ë‹¤. í•˜ì§€ë§Œ ë†’ì€ í•´ìƒë„ì—ì„œëŠ” ì¢‹ì€ featureë“¤ì„ ì–»ì„ ìˆ˜ ì—†ë‹¤. ê·¸ë ‡ê¸°ì— ë†’ì€ í•´ìƒë„ì—ì„œ ê´€ì¸¡ ê°€ëŠ¥í•œ ì‘ì€ ê°ì²´ë“¤ì„ íƒì§€í•˜ê¸° ì–´ë ¤ìš¸ ê²ƒì´ë‹¤.\n  ê·¸ë˜ì„œ ì—¬ê¸°ì„œ í•˜ë‚˜ ë‹¨ì–´ë¥¼ ì¡ê³  ê°€ë©´ **Semantic** Gapì´ë‹¤.\n  ![img](https://cdn.discordapp.com/attachments/904717831940239403/932675643722838077/unknown.png)\n- **Feature Pyramid Network**\n  ![img](https://cdn.discordapp.com/attachments/904717831940239403/932676070908502046/unknown.png)\n  ê·¸ë˜ì„œ ë‚˜ì˜¨ê²Œ FPNì´ë‹¤.  ê·¸ëŸ¬ë©´ ë´ì•¼í•˜ëŠ” íŠ¹ì§•ì´ ì„¸ ê°€ì§€ ìˆë‹¤.(Yolov3 ëª¨ë¸ë¡œ ì„¤ëª…í•  ê²ƒì´ë‹¤. ì‚¬ì‹¤ ì™„ë²½í•œ FPNì€ ì•„ë‹Œ ê²ƒ ê°™ë‹¤.)\n  - bottom-up pathway\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932682821896667236/unknown.png)\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932682880541392977/unknown.png)\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932678088419061810/unknown.png)\n    í•´ë‹¹ ë¶€ë¶„ì€ downsampling ê³¼ì •ì´ê³  í•´ìƒë„ë¥¼ ë‚®ì¶”ë©´ì„œ feature levelì„ ë†’ì´ëŠ” ë‹¨ê³„ì´ë‹¤. FNP ë…¼ë¬¸ì—ì„œëŠ” ResNetì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…í–ˆë‹¤.\n  - top-down pathway\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932678088419061810/unknown.png)\n    í•´ë‹¹ ë¶€ë¶„ì€ upsampling ê³¼ì •ì´ë‹¤. í•´ìƒë„ë¥¼ ë†’ì´ëŠ”ë° featureë¥¼ ê°–ê³  ê°„ë‹¤. ë°©ë²•ì€ nn.Upsampling ê³µì‹ ë¬¸ì„œë¥¼ í™•ì¸í•´ë„ ë˜ì§€ë§Œ ê°„ëµíˆ ì„¤ëª…í•˜ë©´ neareastëŠ” ì•„ë˜ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë™ì‘í•œë‹¤.\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932680429964787712/unknown.png)\n  - lateral connections\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932683196317970502/unknown.png)\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932678851274879056/unknown.png)\n    ì—¬ê¸° ë¶€ë¶„ì´ yolov3ì™€ ë‹¤ë¥´ë‹¤. FNPëŠ” +ë¥¼ í•œë‹¤. í•˜ì§€ë§Œ yolov3ëŠ” depthê¸°ì¤€ concatì„ ìˆ˜í–‰í•œë‹¤.\n    ![img](https://cdn.discordapp.com/attachments/904717831940239403/932681564372992052/unknown.png)\n    í•´ë‹¹ ë¶€ë¶„ì´ ì–´ë–¤ ì˜ë¯¸ê°€ ìˆëŠ”ì§€ëŠ” ì¢€ ë” ì°¾ì•„ë³´ê² ë‹¤.\n    ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì€ + ì—°ì‚°ìœ¼ë¡œ 1x1 convì—ì„œ output ê°’ì„ ë§ì¶°ì¤€ë‹¤. ë”°ë¼ì„œ ê°™ì€ shapeë¡œ + ì—°ì‚°ì´ ì´ë£¨ì–´ì§„ë‹¤.\n    ![img](https://blog.kakaocdn.net/dn/0ela2/btqUsdFXuAe/zSFO8k1p1JIbMoz5vWi75k/img.png)\n\n\n\n\n\n## Region Proposal Network(RPN)\n\ní•´ë‹¹ ë‚´ìš©ì€ **Faster-RCNN**ì—ì„œ ë‚˜ì˜¨ ë‚´ìš©ì´ë‹¤. **Yolov2**ì—ì„œ anchorëŠ” ìœ„ ë…¼ë¬¸ì„ ë³´ê³  ë”°ì™”ë‹¤. \n\n```bash\n3.1 Region Proposal Networks\nA Region Proposal Network (RPN) takes an image\n(of any size) as input and outputs a set of rectangular\nobject proposals, each with an objectness score.3 We\nmodel this process with a fully convolutional network\n[7], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN\nobject detection network [2], we assume that both nets\nshare a common set of convolutional layers. In our experiments, we investigate the Zeiler and Fergus model\n[32] (ZF), which has 5 shareable convolutional layers\nand the Simonyan and Zisserman model [3] (VGG-16),\nwhich has 13 shareable convolutional layers.\nTo generate region proposals, we slide a small\nnetwork over the convolutional feature map output\nby the last shared convolutional layer. This small\nnetwork takes as input an n Ã— n spatial window of\nthe input convolutional feature map. Each sliding\nwindow is mapped to a lower-dimensional feature\n(256-d for ZF and 512-d for VGG, with ReLU [33]\nfollowing). This feature is fed into two sibling fullyconnected layersâ€”a box-regression layer (reg) and a\nbox-classification layer (cls). We use n = 3 in this\npaper, noting that the effective receptive field on the\ninput image is large (171 and 228 pixels for ZF and\nVGG, respectively). This mini-network is illustrated\nat a single position in Figure 3 (left). Note that because the mini-network operates in a sliding-window\nfashion, the fully-connected layers are shared across\nall spatial locations. This architecture is naturally implemented with an nÃ—n convolutional layer followed\nby two sibling 1 Ã— 1 convolutional layers (for reg and\ncls, respectively).\n3.1.1 Anchors\nAt each sliding-window location, we simultaneously\npredict multiple region proposals, where the number\nof maximum possible proposals for each location is\ndenoted as k. So the reg layer has 4k outputs encoding\nthe coordinates of k boxes, and the cls layer outputs\n2k scores that estimate probability of object or not\nobject for each proposal4\n. The k proposals are parameterized relative to k reference boxes, which we call\nanchors. An anchor is centered at the sliding window\nin question, and is associated with a scale and aspect\nratio (Figure 3, left). By default we use 3 scales and\n3 aspect ratios, yielding k = 9 anchors at each sliding\nposition. For a convolutional feature map of a size\nW Ã— H (typically âˆ¼2,400), there are W Hk anchors in\ntotal.\n```\n","fields":{"timeToRead":{"minutes":15.37},"slug":"/posts/2022-02-10/fpn/"},"internal":{"contentFilePath":"/Users/jeongdongha/code/toy/ha4219.github.io/contents/posts/2022-02-10/fpn.mdx"}},"next":null,"previous":{"fields":{"slug":"/posts/2022-03-01/test-algorithm/"}}}]}},"staticQueryHashes":[],"slicesMap":{}}